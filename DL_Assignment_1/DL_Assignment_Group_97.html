<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>dl_assignment_group_97</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="DL_Assignment_Group_97_files/libs/clipboard/clipboard.min.js"></script>
<script src="DL_Assignment_Group_97_files/libs/quarto-html/quarto.js"></script>
<script src="DL_Assignment_Group_97_files/libs/quarto-html/popper.min.js"></script>
<script src="DL_Assignment_Group_97_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="DL_Assignment_Group_97_files/libs/quarto-html/anchor.min.js"></script>
<link href="DL_Assignment_Group_97_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="DL_Assignment_Group_97_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="DL_Assignment_Group_97_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="DL_Assignment_Group_97_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="DL_Assignment_Group_97_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">



<hr>
<p>title:DL Assignment format: pdf jupyter: python3 ___</p>
<section id="group-no-97" class="level2">
<h2 class="anchored" data-anchor-id="group-no-97">Group No 97</h2>
</section>
<section id="group-member-names" class="level2">
<h2 class="anchored" data-anchor-id="group-member-names">Group Member Names:</h2>
<table class="table">
<thead>
<tr class="header">
<th>No.</th>
<th>Member</th>
<th>Student ID</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Shreysi Kalra</td>
<td>2021fc04586</td>
</tr>
<tr class="even">
<td>2</td>
<td>Vinayak Nayak</td>
<td>2021fc04135</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Ajith Praveen R</td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="problem-statement" class="level1">
<h1>1. Problem Statement</h1>
<p>Students are expected to identify a classification / regression problem of your choice. You have to detail the problem under this heading which basically addresses the following questions.</p>
<ol type="1">
<li>What is the problem that you are trying to solve?</li>
<li>What kind of prediction (classification / regression) task are you performing?</li>
</ol>
<p>ENSURE THAT YOU ARE USING NUMERICAL / CATEGORICAL DATA only.</p>
<p>DO NOT use images or textual data.</p>
<p>Score: 1 Mark in total (0.5 mark each)</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>head <span class="op">-</span>n <span class="dv">10</span> .<span class="op">/</span>StudentsPerformance.csv</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>"gender","race/ethnicity","parental level of education","lunch","test preparation course","math score","reading score","writing score"
"female","group B","bachelor's degree","standard","none","72","72","74"
"female","group C","some college","standard","completed","69","90","88"
"female","group B","master's degree","standard","none","90","95","93"
"male","group A","associate's degree","free/reduced","none","47","57","44"
"male","group C","some college","standard","none","76","78","75"
"female","group B","associate's degree","standard","none","71","83","78"
"female","group B","some college","standard","completed","88","95","92"
"male","group B","some college","free/reduced","none","40","43","39"
"male","group D","high school","free/reduced","completed","64","64","67"</code></pre>
</div>
</div>
<section id="student-performance-prediction" class="level3">
<h3 class="anchored" data-anchor-id="student-performance-prediction">Student Performance Prediction</h3>
<p><img src="https://images.pexels.com/photos/249360/pexels-photo-249360.jpeg?auto=compress&amp;cs=tinysrgb&amp;w=500" style="margin-left:auto;margin-right:auto"></p>
<div class="alert alert-primary" style="padding: 10px; background-color: #a5eaf2; color: gray;">
<p>In this problem, we are trying to predict the scores of students in three tests given information regarding their demographic information, academic attributes and their preparedness for the tests.<br> The intention here is to figure out the performance beforehand and use that as a pre-emptive measure to bolster the student’s aptitude in places where he needs help.<br> Since the scores are continuous in nature, we are going to model this as a regression problem. As we have three test scores to predict, we will have 3 outputs and we have 5 attributes/features.</p>
</div>
</section>
</section>
<section id="data-acquisition" class="level1">
<h1>2. Data Acquisition</h1>
<p>For the problem identified by you, students have to find the data source themselves from any data source.</p>
<section id="download-the-data-directly" class="level2">
<h2 class="anchored" data-anchor-id="download-the-data-directly">2.1 Download the data directly</h2>
<p>Since we’re downloading data from kaggle, we will use the kaggle API to fetch the data. For that we will need to install the kaggle library and set our credentials in order to use it. Follow <a href="https://www.kaggle.com/docs/api#getting-started-installation-&amp;-authentication">these steps</a> in order to authenticate and create a new API Token to use the kaggle python library for downloading the dataset.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install <span class="op">-</span>q kaggle</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Take a look at your credentials file </span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !cat ~/.kaggle/kaggle.json</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Protect your authentication file against accidental overwriting by only allowing read access to the file</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !chmod +400 ~/.kaggle/kaggle.json</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Download the dataset from kaggle</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !kaggle datasets download -d spscientist/students-performance-in-exams</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Unip the downloaded dataset and delete the zip</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !unzip -q students-performance-in-exams.zip</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># !rm students-performance-in-exams.zip</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="code-for-converting-the-above-downloaded-data-into-a-form-suitable-for-dl" class="level2">
<h2 class="anchored" data-anchor-id="code-for-converting-the-above-downloaded-data-into-a-form-suitable-for-dl">2.2 Code for converting the above downloaded data into a form suitable for DL</h2>
<div class="alert alert-primary" style="padding: 10px; background-color: #a5eaf2; color: gray;">
<p>The data is in the form of a csv file and can be conveniently used for building our deep learning regression model</p>
</div>
</section>
<section id="write-your-observations-from-the-above." class="level2">
<h2 class="anchored" data-anchor-id="write-your-observations-from-the-above.">2.3 Write your observations from the above.</h2>
<ol type="1">
<li>Size of the dataset</li>
<li>What type of data attributes are there?</li>
</ol>
<p>Score: 2 Mark</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the size of the dataset</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>ls <span class="op">-</span>alth StudentsPerformance.csv</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>-rw-rw-r-- 1 vinayak vinayak 71K Oct 11  2019 StudentsPerformance.csv</code></pre>
</div>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of records in the dataset</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>cat StudentsPerformance.csv<span class="op">|</span> wc <span class="op">-</span>l</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1001</code></pre>
</div>
</div>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Peek at the header and first row of the dataset</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>head <span class="op">-</span>n <span class="dv">2</span> StudentsPerformance.csv</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>"gender","race/ethnicity","parental level of education","lunch","test preparation course","math score","reading score","writing score"
"female","group B","bachelor's degree","standard","none","72","72","74"</code></pre>
</div>
</div>
<div class="alert alert-primary" style="padding: 10px; background-color: #a5eaf2; color: gray;">
<pre><code>&lt;b&gt;Size of dataset&lt;/b&gt;</code></pre>
<ul>
<li>
Disk Space occupied: 71KB
</li>
<li>
Total Number of records: 1000
</li>
</ul>
<pre><code>&lt;b&gt;Attribute information is as follows&lt;/b&gt;</code></pre>
</div>
<table class="table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Attribute Name</th>
<th>Description</th>
<th>Attribute Type</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Gender</td>
<td>Gender of the subject</td>
<td>Nominal</td>
</tr>
<tr class="even">
<td>Race/Ethnicity</td>
<td>Race to which the subject belongs(masked)</td>
<td>Nominal</td>
</tr>
<tr class="odd">
<td>Parental Level of Education</td>
<td>Extent of education</td>
<td>Ordinal</td>
</tr>
<tr class="even">
<td>Lunch</td>
<td>Kind of lunch opted for</td>
<td>Nominal</td>
</tr>
<tr class="odd">
<td>Test Prep Course</td>
<td>Whether or not preparation for the test is done</td>
<td>Ordinal</td>
</tr>
<tr class="even">
<td>Math Score</td>
<td>Marks obtained in math</td>
<td>Numeric</td>
</tr>
<tr class="odd">
<td>Reading Score</td>
<td>Marks obtained in reading</td>
<td>Numeric</td>
</tr>
<tr class="even">
<td>Writing Score</td>
<td>Marks obtained in writing</td>
<td>Numeric</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="data-preparation" class="level1">
<h1>3. Data Preparation</h1>
<p>Perform the data prepracessing that is required for the data that you have downloaded.</p>
<section id="apply-techiniques" class="level2">
<h2 class="anchored" data-anchor-id="apply-techiniques">3.1 Apply techiniques</h2>
<ul>
<li>to remove duplicate data</li>
<li>to impute or remove missing data</li>
<li>to remove data inconsistencies</li>
</ul>
<p>IF ANY</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install <span class="op">-</span>q seaborn pandas numpy matplotlib </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">"ggplot"</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Read the data into a dataframe</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">"StudentsPerformance.csv"</span>)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>df.head(<span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>gender</th>
      <th>race/ethnicity</th>
      <th>parental level of education</th>
      <th>lunch</th>
      <th>test preparation course</th>
      <th>math score</th>
      <th>reading score</th>
      <th>writing score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>female</td>
      <td>group B</td>
      <td>bachelor's degree</td>
      <td>standard</td>
      <td>none</td>
      <td>72</td>
      <td>72</td>
      <td>74</td>
    </tr>
    <tr>
      <th>1</th>
      <td>female</td>
      <td>group C</td>
      <td>some college</td>
      <td>standard</td>
      <td>completed</td>
      <td>69</td>
      <td>90</td>
      <td>88</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Peek at all the column names</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Dataset columns</span><span class="ch">\n</span><span class="sc">{</span>df<span class="sc">.</span>columns<span class="sc">.</span>tolist()<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Dataset columns
['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course', 'math score', 'reading score', 'writing score']</code></pre>
</div>
</div>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize <span class="op">=</span> (<span class="dv">10</span>,<span class="dv">3</span>), sharex <span class="op">=</span> <span class="va">True</span>, sharey <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].hist(df[<span class="st">"math score"</span>])<span class="op">;</span> ax[<span class="dv">0</span>].set_title(<span class="ss">f"Math Score Distribution"</span>)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].hist(df[<span class="st">"reading score"</span>])<span class="op">;</span> ax[<span class="dv">1</span>].set_title(<span class="ss">f"Reading Score Distribution"</span>)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].hist(df[<span class="st">"writing score"</span>])<span class="op">;</span> ax[<span class="dv">2</span>].set_title(<span class="ss">f"Writing Score Distribution"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="DL_Assignment_Group_97_files/figure-html/cell-14-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize <span class="op">=</span> (<span class="dv">10</span>,<span class="dv">3</span>))</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>sns.countplot(x <span class="op">=</span> df[<span class="st">"gender"</span>], ax <span class="op">=</span> ax[<span class="dv">0</span>])<span class="op">;</span> ax[<span class="dv">0</span>].set_title(<span class="st">"Gender Distribution"</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>sns.countplot(x <span class="op">=</span> df[<span class="st">"race/ethnicity"</span>], ax <span class="op">=</span> ax[<span class="dv">1</span>])<span class="op">;</span> ax[<span class="dv">1</span>].set_title(<span class="st">"Race Distribution"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="DL_Assignment_Group_97_files/figure-html/cell-15-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize <span class="op">=</span> (<span class="dv">10</span>,<span class="dv">3</span>))</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>sns.countplot(x <span class="op">=</span> df[<span class="st">"parental level of education"</span>], ax <span class="op">=</span> ax[<span class="dv">0</span>])<span class="op">;</span> ax[<span class="dv">0</span>].set_title(<span class="st">"Education Level of parent"</span>)<span class="op">;</span> ax[<span class="dv">0</span>].tick_params(axis<span class="op">=</span><span class="st">'x'</span>, rotation<span class="op">=</span><span class="dv">90</span>)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>sns.countplot(x <span class="op">=</span> df[<span class="st">"lunch"</span>], ax <span class="op">=</span> ax[<span class="dv">1</span>])<span class="op">;</span> ax[<span class="dv">1</span>].set_title(<span class="st">"Lunch Distribution"</span>)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>sns.countplot(x <span class="op">=</span> df[<span class="st">"test preparation course"</span>], ax <span class="op">=</span> ax[<span class="dv">2</span>])<span class="op">;</span> ax[<span class="dv">2</span>].set_title(<span class="st">"Preparation Course"</span>)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="DL_Assignment_Group_97_files/figure-html/cell-16-output-1.png" class="img-fluid"></p>
</div>
</div>
<ul>
<li><code>Gender</code>, <code>Lunch Distribution</code> and <code>Preparation Course</code> are binary variables with a very slight imbalance in their levels (in the latter 2)</li>
<li><code>Parent's education</code> is a six level ordinal variable with a relatively small number of parents having bachelor’s or master’s degree</li>
<li><code>Race</code> is also an imbalanced distribution with five levels (Masked).</li>
</ul>
<p>It seems like the scores are near normal distributed with a slight left skew.</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>df.isnull().<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>gender                         0
race/ethnicity                 0
parental level of education    0
lunch                          0
test preparation course        0
math score                     0
reading score                  0
writing score                  0
dtype: int64</code></pre>
</div>
</div>
<p>None of the columns in the dataset are missing. So, we don’t need to impute anything or substitute for any record across any feature in the dataset.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>df.duplicated().<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>0</code></pre>
</div>
</div>
<p>None of the records seem to be duplicated. All rows are unique instances. Hence keep the records as is.</p>
</section>
<section id="encode-categorical-data" class="level2">
<h2 class="anchored" data-anchor-id="encode-categorical-data">3.2 Encode categorical data</h2>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install <span class="op">-</span>q scikit<span class="op">-</span>learn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><p>The binary attributes are all nominal in nature and can be simply encoded in a <code>1/0</code> fashion.</p></li>
<li><p><code>Race</code> is a nominal attribute which could be one-hot-encoded</p></li>
<li><p><code>Parent's education</code> is an ordinal attribute and can be label encoded.</p></li>
</ul>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col, level <span class="kw">in</span> [(<span class="st">"gender"</span>, <span class="st">"male"</span>), (<span class="st">"lunch"</span>, <span class="st">"standard"</span>), (<span class="st">"test preparation course"</span>, <span class="st">"none"</span>)]:</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    df[col] <span class="op">=</span> df[col].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="fl">0.</span> <span class="cf">if</span> x <span class="op">==</span> level <span class="cf">else</span> <span class="fl">1.</span>)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>df.head(<span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>gender</th>
      <th>race/ethnicity</th>
      <th>parental level of education</th>
      <th>lunch</th>
      <th>test preparation course</th>
      <th>math score</th>
      <th>reading score</th>
      <th>writing score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>group B</td>
      <td>bachelor's degree</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>72</td>
      <td>72</td>
      <td>74</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.0</td>
      <td>group C</td>
      <td>some college</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>69</td>
      <td>90</td>
      <td>88</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>df[<span class="st">"parental level of education"</span>].unique()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>array(["bachelor's degree", 'some college', "master's degree",
       "associate's degree", 'high school', 'some high school'],
      dtype=object)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>education_order <span class="op">=</span> {<span class="st">"high school"</span>: <span class="dv">0</span>, <span class="st">"some high school"</span>: <span class="dv">0</span>, <span class="st">"some college"</span>: <span class="dv">1</span>, </span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>                   <span class="st">"associate's degree"</span>: <span class="dv">2</span>, <span class="st">"bachelor's degree"</span>: <span class="dv">3</span>, <span class="st">"master's degree"</span>: <span class="dv">4</span>}</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>df[<span class="st">"parental level of education"</span>] <span class="op">=</span> df[<span class="st">"parental level of education"</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: education_order[x])</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>df.head(<span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>gender</th>
      <th>race/ethnicity</th>
      <th>parental level of education</th>
      <th>lunch</th>
      <th>test preparation course</th>
      <th>math score</th>
      <th>reading score</th>
      <th>writing score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>group B</td>
      <td>3</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>72</td>
      <td>72</td>
      <td>74</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.0</td>
      <td>group C</td>
      <td>1</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>69</td>
      <td>90</td>
      <td>88</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>Now let’s one hot encode the categorical column for race. We shall drop the first race to avoid redundancy</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> OneHotEncoder</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>ohe <span class="op">=</span> OneHotEncoder(drop <span class="op">=</span> <span class="st">"first"</span>)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>df_encoded <span class="op">=</span> pd.DataFrame(ohe.fit_transform(df[[<span class="st">"race/ethnicity"</span>]]).toarray(), columns <span class="op">=</span> ohe.get_feature_names_out())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>df_processed <span class="op">=</span> pd.concat([df, df_encoded], axis <span class="op">=</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>df_processed</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>gender</th>
      <th>race/ethnicity</th>
      <th>parental level of education</th>
      <th>lunch</th>
      <th>test preparation course</th>
      <th>math score</th>
      <th>reading score</th>
      <th>writing score</th>
      <th>race/ethnicity_group B</th>
      <th>race/ethnicity_group C</th>
      <th>race/ethnicity_group D</th>
      <th>race/ethnicity_group E</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>group B</td>
      <td>3</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>72</td>
      <td>72</td>
      <td>74</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.0</td>
      <td>group C</td>
      <td>1</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>69</td>
      <td>90</td>
      <td>88</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.0</td>
      <td>group B</td>
      <td>4</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>90</td>
      <td>95</td>
      <td>93</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.0</td>
      <td>group A</td>
      <td>2</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>47</td>
      <td>57</td>
      <td>44</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.0</td>
      <td>group C</td>
      <td>1</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>76</td>
      <td>78</td>
      <td>75</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>995</th>
      <td>1.0</td>
      <td>group E</td>
      <td>4</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>88</td>
      <td>99</td>
      <td>95</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>996</th>
      <td>0.0</td>
      <td>group C</td>
      <td>0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>62</td>
      <td>55</td>
      <td>55</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>997</th>
      <td>1.0</td>
      <td>group C</td>
      <td>0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>59</td>
      <td>71</td>
      <td>65</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>998</th>
      <td>1.0</td>
      <td>group D</td>
      <td>1</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>68</td>
      <td>78</td>
      <td>77</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>999</th>
      <td>1.0</td>
      <td>group D</td>
      <td>1</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>77</td>
      <td>86</td>
      <td>86</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>1000 rows × 12 columns</p>
</div>
</div>
</div>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>df_processed.drop([<span class="st">"race/ethnicity"</span>], inplace <span class="op">=</span> <span class="va">True</span>, axis <span class="op">=</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="normalize-the-data" class="level2">
<h2 class="anchored" data-anchor-id="normalize-the-data">3.3 Normalize the data</h2>
<p>Out targets are numerical. We shall normalize them to lie in the range of 0-1.</p>
<p>Since we’ve seen that the range of marks is from 0 to 100, we shall simply divide by 100 to bring the data in 0-1 range.</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>df.columns</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>Index(['gender', 'race/ethnicity', 'parental level of education', 'lunch',
       'test preparation course', 'math score', 'reading score',
       'writing score'],
      dtype='object')</code></pre>
</div>
</div>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> [<span class="st">"math score"</span>, <span class="st">"reading score"</span>, <span class="st">"writing score"</span>]:</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>    df_processed[col] <span class="op">=</span> df_processed[col] <span class="op">/</span> <span class="dv">100</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>df_processed.head(<span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>gender</th>
      <th>parental level of education</th>
      <th>lunch</th>
      <th>test preparation course</th>
      <th>math score</th>
      <th>reading score</th>
      <th>writing score</th>
      <th>race/ethnicity_group B</th>
      <th>race/ethnicity_group C</th>
      <th>race/ethnicity_group D</th>
      <th>race/ethnicity_group E</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>3</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.72</td>
      <td>0.72</td>
      <td>0.74</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.0</td>
      <td>1</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.69</td>
      <td>0.90</td>
      <td>0.88</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>We could also normalize the parental level of education to lie between 0-1 so it would be easier for the network when learning</p>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>df_processed[<span class="st">"parental level of education"</span>] <span class="op">=</span> df_processed[<span class="st">"parental level of education"</span>] <span class="op">/</span> <span class="dv">4</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="feature-engineering" class="level2">
<h2 class="anchored" data-anchor-id="feature-engineering">3.4 Feature Engineering</h2>
<p>if any</p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># In this problem, there doesn't seem to be much scope for feature engineering</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>df_processed.head(<span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="29">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>gender</th>
      <th>parental level of education</th>
      <th>lunch</th>
      <th>test preparation course</th>
      <th>math score</th>
      <th>reading score</th>
      <th>writing score</th>
      <th>race/ethnicity_group B</th>
      <th>race/ethnicity_group C</th>
      <th>race/ethnicity_group D</th>
      <th>race/ethnicity_group E</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>0.75</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.72</td>
      <td>0.72</td>
      <td>0.74</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.0</td>
      <td>0.25</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.69</td>
      <td>0.90</td>
      <td>0.88</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</section>
<section id="identify-the-target-variables." class="level2">
<h2 class="anchored" data-anchor-id="identify-the-target-variables.">3.5 Identify the target variables.</h2>
<ul>
<li><p>Separate the data front the target such that the dataset is in the form of (X,y) or (Features, Label)</p></li>
<li><p>Discretize / Encode the target variable or perform one-hot encoding on the target or any other as and if required.</p></li>
</ul>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>target_variables <span class="op">=</span> [<span class="st">"math score"</span>, <span class="st">"reading score"</span>, <span class="st">"writing score"</span>]</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>attribute_variables <span class="op">=</span> [x <span class="cf">for</span> x <span class="kw">in</span> df_processed.columns.tolist() <span class="cf">if</span> x <span class="kw">not</span> <span class="kw">in</span> target_variables]</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df_processed[attribute_variables].values</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df_processed[target_variables].values</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Have a look at the X and y variables</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>X, y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>(array([[1.  , 0.75, 0.  , ..., 0.  , 0.  , 0.  ],
        [1.  , 0.25, 0.  , ..., 1.  , 0.  , 0.  ],
        [1.  , 1.  , 0.  , ..., 0.  , 0.  , 0.  ],
        ...,
        [1.  , 0.  , 1.  , ..., 1.  , 0.  , 0.  ],
        [1.  , 0.25, 0.  , ..., 0.  , 1.  , 0.  ],
        [1.  , 0.25, 1.  , ..., 0.  , 1.  , 0.  ]]),
 array([[0.72, 0.72, 0.74],
        [0.69, 0.9 , 0.88],
        [0.9 , 0.95, 0.93],
        ...,
        [0.59, 0.71, 0.65],
        [0.68, 0.78, 0.77],
        [0.77, 0.86, 0.86]]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Inspect the shape of both these arrays</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>X.shape, y.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>((1000, 8), (1000, 3))</code></pre>
</div>
</div>
</section>
<section id="split-the-data-into-training-set-and-testing-set" class="level2">
<h2 class="anchored" data-anchor-id="split-the-data-into-training-set-and-testing-set">3.6 Split the data into training set and testing set</h2>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, random_state <span class="op">=</span> <span class="dv">42</span>, test_size <span class="op">=</span> <span class="fl">0.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>X_train.shape, y_train.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>((800, 8), (800, 3))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>X_test.shape, y_test.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>((200, 8), (200, 3))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make sure that the train and test set sizes are 80% and 20% of the entire data respectively</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> X_train.shape[<span class="dv">0</span>] <span class="op">==</span> <span class="bu">len</span>(X) <span class="op">*</span> <span class="fl">0.8</span></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> X_test.shape[<span class="dv">0</span>] <span class="op">==</span> <span class="bu">len</span>(X) <span class="op">*</span> <span class="fl">0.2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="report" class="level2">
<h2 class="anchored" data-anchor-id="report">3.7 Report</h2>
<p>Mention the method adopted and justify why the method was used * to remove duplicate data, if present * to impute or remove missing data, if present * to remove data inconsistencies, if present * to encode categorical data * the normalization technique used</p>
<p>If the any of the above are not present, then also add in the report below.</p>
<p>Report the size of the training dataset and testing dataset</p>
<p>Score: 3 Marks</p>
<div class="alert alert-primary" style="padding: 10px; background-color: #a5eaf2; color: gray;">
<pre><code>&lt;b&gt;Data Deduplication&lt;/b&gt;&lt;br&gt;
&lt;span&gt;None, the data had no duplicates&lt;/span&gt;&lt;br&gt;
&lt;b&gt;Missing Data Imputation&lt;/b&gt;&lt;br&gt;
&lt;span&gt;None of the instances had a single attibute missing.&lt;/span&gt;&lt;br&gt;
&lt;b&gt;Data Inconsistency&lt;/b&gt;&lt;br&gt;
&lt;span&gt;Provided data seemed consistent.&lt;/span&gt;&lt;br&gt;
&lt;b&gt;Categorical Data Encoding&lt;/b&gt;&lt;br&gt;
&lt;ul&gt;
    &lt;li&gt;The binary attributes are all nominal in nature and can be simply encoded in a `1/0` fashion.&lt;/li&gt;
    &lt;li&gt;`Race` is a nominal attribute which is one-hot-encoded&lt;/li&gt;
    &lt;li&gt;`Parent's education` is an ordinal attribute and is label encoded.&lt;/li&gt;
&lt;/ul&gt;
&lt;b&gt;Normalization Technique Used&lt;/b&gt;&lt;br&gt;
&lt;span&gt;Division by a hundred. Since we knew the range of data to begin with.&lt;/span&gt;&lt;br&gt;
&lt;b&gt;Dataset Sizes&lt;/b&gt;&lt;br&gt;
&lt;span&gt;Train: 800  | Test: 200&lt;/span&gt;</code></pre>
</div>
</section>
</section>
<section id="deep-neural-network-architecture" class="level1">
<h1>4. Deep Neural Network Architecture</h1>
<section id="design-the-architecture-that-you-will-be-using-to-solve-the-prediction-problem-identified." class="level2">
<h2 class="anchored" data-anchor-id="design-the-architecture-that-you-will-be-using-to-solve-the-prediction-problem-identified.">4.1 Design the architecture that you will be using to solve the prediction problem identified.</h2>
<ul>
<li>Add dense layers, specifying the number of units in each layer and the activation function used in the layer.</li>
</ul>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!pip install tensorflow</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> layers</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.regularizers <span class="im">import</span> l1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>2022-12-03 21:46:21.969016: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-03 21:46:22.265398: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64
2022-12-03 21:46:22.265419: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2022-12-03 21:46:22.875893: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64
2022-12-03 21:46:22.875979: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64
2022-12-03 21:46:22.875988: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.</code></pre>
</div>
</div>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>X_train.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>(800, 8)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> tf.keras.Sequential([ layers.Input(X_train.shape[<span class="op">-</span><span class="dv">1</span>],),</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>                              layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>, kernel_regularizer <span class="op">=</span> l1(<span class="fl">1e-4</span>)),</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>                              layers.BatchNormalization(), layers.Dropout(<span class="fl">.1</span>),</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>                              layers.Dense(<span class="dv">32</span>, activation <span class="op">=</span> <span class="st">'relu'</span>),</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>                              layers.Dense(<span class="dv">3</span>, activation <span class="op">=</span> <span class="st">'linear'</span>)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>2022-12-03 21:46:23.706042: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-03 21:46:23.707153: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64
2022-12-03 21:46:23.707373: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64
2022-12-03 21:46:23.707582: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64
2022-12-03 21:46:23.712396: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64
2022-12-03 21:46:23.712630: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64
2022-12-03 21:46:23.712847: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64
2022-12-03 21:46:23.712888: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2022-12-03 21:46:23.713632: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.</code></pre>
</div>
</div>
<div class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>model.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 64)                576       
                                                                 
 batch_normalization (BatchN  (None, 64)               256       
 ormalization)                                                   
                                                                 
 dropout (Dropout)           (None, 64)                0         
                                                                 
 dense_1 (Dense)             (None, 32)                2080      
                                                                 
 dense_2 (Dense)             (None, 3)                 99        
                                                                 
=================================================================
Total params: 3,011
Trainable params: 2,883
Non-trainable params: 128
_________________________________________________________________</code></pre>
</div>
</div>
</section>
<section id="report-1" class="level2">
<h2 class="anchored" data-anchor-id="report-1">4.2 Report</h2>
<p>Report the following and provide justification for the same.</p>
<ul>
<li>Number of layers</li>
<li>Number of units in each layer</li>
<li>Activation function used in each hidden layer</li>
<li>Activation function used in the output layer</li>
<li>Total number of trainable parameters</li>
</ul>
<p>Score: 4 Marks</p>
<p>We have defined a two layer model as follows</p>
<p><img src="neural_network_design.png" class="img-fluid"></p>
<div class="alert alert-primary" style="padding: 10px; background-color: #a5eaf2; color: gray;">
<pre><code>&lt;b&gt;Number of layers&lt;/b&gt;&lt;br&gt;
&lt;span&gt;Our architecture is as defined above: 1 input, 2 hidden and 1 output layer&lt;/span&gt;&lt;br&gt;
&lt;b&gt;Number of units in each layer&lt;/b&gt;
&lt;ul&gt;
    &lt;li&gt;Input layer - 8 neurons&lt;/li&gt;
    &lt;li&gt;First Hidden Layer - 64 neurons&lt;/li&gt;
    &lt;li&gt;Batch Normalization Layer - 64 neurons&lt;/li&gt;
    &lt;li&gt;Dropout with p = 10%&lt;/li&gt;
    &lt;li&gt;Second Hidden Layer - 64 neurons&lt;/li&gt;
    &lt;li&gt;Output Layer - 3 neurons&lt;/li&gt;
&lt;/ul&gt;
&lt;b&gt;Activation function in hidden layers&lt;/b&gt;&lt;br&gt;
&lt;span&gt;In both hidden layers, ReLU is used as activation. It is easy to implement, differentiable, does not saturate unlike sigmoid and as we don't have any constraint to model the output as a probability distribution or anything in the hidden layer output, this activation function makes sense.&lt;/span&gt;&lt;br&gt;
&lt;b&gt;Activation function in output layer&lt;/b&gt;&lt;br&gt;
&lt;span&gt;Linear Activation function is used in the output layer. The output although constrained with 0-1 in this case (in training data) has a continuous representation and no notion of probability to it. Hence this activation function&lt;/span&gt;&lt;br&gt;
&lt;b&gt;Total number of trainable parameters&lt;/b&gt; </code></pre>
</div>
<p><img src="param_calculation.png"></p>
</section>
</section>
<section id="training-the-model" class="level1">
<h1>5. Training the model</h1>
<section id="configure-the-training" class="level2">
<h2 class="anchored" data-anchor-id="configure-the-training">5.1 Configure the training</h2>
<p>Configure the model for training, by using appropriate optimizers and regularizations</p>
<div class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> RMSE(y_true, y_pred):</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tf.py_function(partial(mean_squared_error, squared<span class="op">=</span><span class="va">False</span>), (y_true, y_pred), tf.double)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.optimizers <span class="im">import</span> SGD, Adam</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_model(optim, lr, bs, eps):</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define the model architecture</span></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> tf.keras.Sequential([ layers.Input(X_train.shape[<span class="op">-</span><span class="dv">1</span>],),</span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>                              layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>, kernel_regularizer <span class="op">=</span> l1(<span class="fl">1e-4</span>)),</span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a>                              layers.BatchNormalization(), layers.Dropout(<span class="fl">.1</span>),</span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a>                              layers.Dense(<span class="dv">32</span>, activation <span class="op">=</span> <span class="st">'relu'</span>),</span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a>                              layers.Dense(<span class="dv">3</span>, activation <span class="op">=</span> <span class="st">'linear'</span>)])</span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define optimizer, learning rate and metric</span></span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> optim <span class="op">==</span> <span class="st">"SGD"</span>:</span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a>        model.<span class="bu">compile</span>(optimizer<span class="op">=</span>SGD(learning_rate <span class="op">=</span> lr), </span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a>                      loss<span class="op">=</span>tf.keras.losses.MeanSquaredError(), </span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a>                      metrics<span class="op">=</span>[RMSE])</span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true" tabindex="-1"></a>        model.<span class="bu">compile</span>(optimizer<span class="op">=</span>Adam(learning_rate <span class="op">=</span> lr), </span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true" tabindex="-1"></a>                      loss<span class="op">=</span>tf.keras.losses.MeanSquaredError(), </span>
<span id="cb64-20"><a href="#cb64-20" aria-hidden="true" tabindex="-1"></a>                      metrics<span class="op">=</span>[RMSE])</span>
<span id="cb64-21"><a href="#cb64-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb64-22"><a href="#cb64-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Train the model</span></span>
<span id="cb64-23"><a href="#cb64-23" aria-hidden="true" tabindex="-1"></a>    history <span class="op">=</span> model.fit(x <span class="op">=</span> X_train, y <span class="op">=</span> y_train, validation_data <span class="op">=</span> (X_test, y_test), epochs <span class="op">=</span> eps, batch_size <span class="op">=</span> bs)</span>
<span id="cb64-24"><a href="#cb64-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model, history</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="train-the-model" class="level2">
<h2 class="anchored" data-anchor-id="train-the-model">5.2 Train the model</h2>
<div class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> itertools <span class="im">import</span> product </span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>optims <span class="op">=</span> [<span class="st">"SGD"</span>, <span class="st">"Adam"</span>]<span class="op">;</span> learning_rates <span class="op">=</span> [<span class="fl">5e-2</span>, <span class="fl">1e-3</span>]<span class="op">;</span> epochs <span class="op">=</span> [<span class="dv">20</span>,<span class="dv">40</span>]<span class="op">;</span> batch_sizes <span class="op">=</span> [<span class="dv">8</span>, <span class="dv">32</span>, <span class="dv">64</span>]</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>hyperparam_combinations <span class="op">=</span> <span class="bu">list</span>(product(optims, learning_rates, epochs, batch_sizes))</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total hyperparam combinations:</span><span class="ch">\n</span><span class="sc">{</span>hyperparam_combinations<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Total hyperparam combinations:
[('SGD', 0.05, 20, 8), ('SGD', 0.05, 20, 32), ('SGD', 0.05, 20, 64), ('SGD', 0.05, 40, 8), ('SGD', 0.05, 40, 32), ('SGD', 0.05, 40, 64), ('SGD', 0.001, 20, 8), ('SGD', 0.001, 20, 32), ('SGD', 0.001, 20, 64), ('SGD', 0.001, 40, 8), ('SGD', 0.001, 40, 32), ('SGD', 0.001, 40, 64), ('Adam', 0.05, 20, 8), ('Adam', 0.05, 20, 32), ('Adam', 0.05, 20, 64), ('Adam', 0.05, 40, 8), ('Adam', 0.05, 40, 32), ('Adam', 0.05, 40, 64), ('Adam', 0.001, 20, 8), ('Adam', 0.001, 20, 32), ('Adam', 0.001, 20, 64), ('Adam', 0.001, 40, 8), ('Adam', 0.001, 40, 32), ('Adam', 0.001, 40, 64)]</code></pre>
</div>
</div>
<div class="cell" data-tags="[]" data-execution_count="45">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> []</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> optim, lr, eps, bs <span class="kw">in</span> hyperparam_combinations:</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">============================ Beginning trial with hyperparams: OPTIMIZER: </span><span class="sc">{</span>optim<span class="sc">}</span><span class="ss">, LEARNING RATE: </span><span class="sc">{</span>lr<span class="sc">}</span><span class="ss">, EPOCHS: </span><span class="sc">{</span>eps<span class="sc">}</span><span class="ss">============================</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> train_model(optim, lr, bs, eps)</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>    models.append(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
============================ Beginning trial with hyperparams: OPTIMIZER: SGD, LEARNING RATE: 0.05, EPOCHS: 20============================

Epoch 1/20
100/100 [==============================] - 1s 6ms/step - loss: 0.1838 - RMSE: 0.3852 - val_loss: 0.1538 - val_RMSE: 0.3776
Epoch 2/20
100/100 [==============================] - 1s 7ms/step - loss: 0.0685 - RMSE: 0.2388 - val_loss: 0.1038 - val_RMSE: 0.3051
Epoch 3/20
100/100 [==============================] - 0s 4ms/step - loss: 0.0525 - RMSE: 0.2062 - val_loss: 0.0598 - val_RMSE: 0.2241
Epoch 4/20
100/100 [==============================] - 1s 6ms/step - loss: 0.0441 - RMSE: 0.1870 - val_loss: 0.0378 - val_RMSE: 0.1700
Epoch 5/20
100/100 [==============================] - 1s 6ms/step - loss: 0.0389 - RMSE: 0.1731 - val_loss: 0.0312 - val_RMSE: 0.1504
Epoch 6/20
100/100 [==============================] - 1s 6ms/step - loss: 0.0367 - RMSE: 0.1672 - val_loss: 0.0297 - val_RMSE: 0.1457
Epoch 7/20
100/100 [==============================] - 1s 6ms/step - loss: 0.0329 - RMSE: 0.1559 - val_loss: 0.0296 - val_RMSE: 0.1458
Epoch 8/20
100/100 [==============================] - 1s 6ms/step - loss: 0.0311 - RMSE: 0.1512 - val_loss: 0.0312 - val_RMSE: 0.1505
Epoch 9/20
100/100 [==============================] - 1s 6ms/step - loss: 0.0299 - RMSE: 0.1471 - val_loss: 0.0294 - val_RMSE: 0.1449
Epoch 10/20
100/100 [==============================] - 1s 5ms/step - loss: 0.0294 - RMSE: 0.1465 - val_loss: 0.0293 - val_RMSE: 0.1444
Epoch 11/20
100/100 [==============================] - 1s 6ms/step - loss: 0.0282 - RMSE: 0.1414 - val_loss: 0.0293 - val_RMSE: 0.1452
Epoch 12/20
100/100 [==============================] - 1s 6ms/step - loss: 0.0277 - RMSE: 0.1413 - val_loss: 0.0294 - val_RMSE: 0.1456
Epoch 13/20
100/100 [==============================] - 1s 7ms/step - loss: 0.0277 - RMSE: 0.1412 - val_loss: 0.0282 - val_RMSE: 0.1410
Epoch 14/20
100/100 [==============================] - 0s 5ms/step - loss: 0.0268 - RMSE: 0.1376 - val_loss: 0.0280 - val_RMSE: 0.1403
Epoch 15/20
100/100 [==============================] - 0s 4ms/step - loss: 0.0261 - RMSE: 0.1358 - val_loss: 0.0287 - val_RMSE: 0.1423
Epoch 16/20
100/100 [==============================] - 1s 6ms/step - loss: 0.0265 - RMSE: 0.1372 - val_loss: 0.0280 - val_RMSE: 0.1407
Epoch 17/20
100/100 [==============================] - 1s 6ms/step - loss: 0.0257 - RMSE: 0.1340 - val_loss: 0.0276 - val_RMSE: 0.1395
Epoch 18/20
100/100 [==============================] - 1s 6ms/step - loss: 0.0255 - RMSE: 0.1340 - val_loss: 0.0276 - val_RMSE: 0.1394
Epoch 19/20
100/100 [==============================] - 1s 5ms/step - loss: 0.0254 - RMSE: 0.1328 - val_loss: 0.0276 - val_RMSE: 0.1392
Epoch 20/20
100/100 [==============================] - 0s 5ms/step - loss: 0.0256 - RMSE: 0.1341 - val_loss: 0.0283 - val_RMSE: 0.1413

============================ Beginning trial with hyperparams: OPTIMIZER: SGD, LEARNING RATE: 0.05, EPOCHS: 20============================

Epoch 1/20
25/25 [==============================] - 1s 13ms/step - loss: 0.3090 - RMSE: 0.4996 - val_loss: 0.1848 - val_RMSE: 0.4014
Epoch 2/20
25/25 [==============================] - 0s 7ms/step - loss: 0.1272 - RMSE: 0.3423 - val_loss: 0.1885 - val_RMSE: 0.4090
Epoch 3/20
25/25 [==============================] - 0s 10ms/step - loss: 0.0953 - RMSE: 0.2938 - val_loss: 0.1781 - val_RMSE: 0.3968
Epoch 4/20
25/25 [==============================] - 0s 9ms/step - loss: 0.0785 - RMSE: 0.2639 - val_loss: 0.1589 - val_RMSE: 0.3757
Epoch 5/20
25/25 [==============================] - 0s 11ms/step - loss: 0.0694 - RMSE: 0.2471 - val_loss: 0.1492 - val_RMSE: 0.3644
Epoch 6/20
25/25 [==============================] - 0s 4ms/step - loss: 0.0693 - RMSE: 0.2470 - val_loss: 0.1386 - val_RMSE: 0.3524
Epoch 7/20
25/25 [==============================] - 0s 7ms/step - loss: 0.0617 - RMSE: 0.2309 - val_loss: 0.1295 - val_RMSE: 0.3406
Epoch 8/20
25/25 [==============================] - 0s 9ms/step - loss: 0.0560 - RMSE: 0.2190 - val_loss: 0.1202 - val_RMSE: 0.3278
Epoch 9/20
25/25 [==============================] - 0s 8ms/step - loss: 0.0546 - RMSE: 0.2158 - val_loss: 0.0989 - val_RMSE: 0.2961
Epoch 10/20
25/25 [==============================] - 0s 10ms/step - loss: 0.0505 - RMSE: 0.2063 - val_loss: 0.0948 - val_RMSE: 0.2889
Epoch 11/20
25/25 [==============================] - 0s 10ms/step - loss: 0.0487 - RMSE: 0.2010 - val_loss: 0.0833 - val_RMSE: 0.2693
Epoch 12/20
25/25 [==============================] - 0s 5ms/step - loss: 0.0451 - RMSE: 0.1934 - val_loss: 0.0713 - val_RMSE: 0.2459
Epoch 13/20
25/25 [==============================] - 0s 3ms/step - loss: 0.0449 - RMSE: 0.1926 - val_loss: 0.0665 - val_RMSE: 0.2370
Epoch 14/20
25/25 [==============================] - 0s 3ms/step - loss: 0.0437 - RMSE: 0.1895 - val_loss: 0.0585 - val_RMSE: 0.2210
Epoch 15/20
25/25 [==============================] - 0s 2ms/step - loss: 0.0445 - RMSE: 0.1917 - val_loss: 0.0526 - val_RMSE: 0.2075
Epoch 16/20
25/25 [==============================] - 0s 3ms/step - loss: 0.0396 - RMSE: 0.1787 - val_loss: 0.0446 - val_RMSE: 0.1891
Epoch 17/20
25/25 [==============================] - 0s 3ms/step - loss: 0.0398 - RMSE: 0.1798 - val_loss: 0.0409 - val_RMSE: 0.1797
Epoch 18/20
25/25 [==============================] - 0s 3ms/step - loss: 0.0377 - RMSE: 0.1738 - val_loss: 0.0377 - val_RMSE: 0.1710
Epoch 19/20
25/25 [==============================] - 0s 3ms/step - loss: 0.0379 - RMSE: 0.1744 - val_loss: 0.0358 - val_RMSE: 0.1663
Epoch 20/20
25/25 [==============================] - 0s 3ms/step - loss: 0.0378 - RMSE: 0.1739 - val_loss: 0.0334 - val_RMSE: 0.1589

============================ Beginning trial with hyperparams: OPTIMIZER: SGD, LEARNING RATE: 0.05, EPOCHS: 20============================

Epoch 1/20
13/13 [==============================] - 1s 23ms/step - loss: 0.4795 - RMSE: 0.6245 - val_loss: 0.2287 - val_RMSE: 0.4558
Epoch 2/20
13/13 [==============================] - 0s 10ms/step - loss: 0.1643 - RMSE: 0.3950 - val_loss: 0.2000 - val_RMSE: 0.4221
Epoch 3/20
13/13 [==============================] - 0s 11ms/step - loss: 0.1301 - RMSE: 0.3472 - val_loss: 0.1797 - val_RMSE: 0.3971
Epoch 4/20
13/13 [==============================] - 0s 11ms/step - loss: 0.1093 - RMSE: 0.3182 - val_loss: 0.1746 - val_RMSE: 0.3923
Epoch 5/20
13/13 [==============================] - 0s 11ms/step - loss: 0.0971 - RMSE: 0.2988 - val_loss: 0.1654 - val_RMSE: 0.3794
Epoch 6/20
13/13 [==============================] - 0s 11ms/step - loss: 0.0904 - RMSE: 0.2857 - val_loss: 0.1591 - val_RMSE: 0.3719
Epoch 7/20
13/13 [==============================] - 0s 10ms/step - loss: 0.0789 - RMSE: 0.2663 - val_loss: 0.1592 - val_RMSE: 0.3722
Epoch 8/20
13/13 [==============================] - 0s 11ms/step - loss: 0.0729 - RMSE: 0.2558 - val_loss: 0.1482 - val_RMSE: 0.3596
Epoch 9/20
13/13 [==============================] - 0s 11ms/step - loss: 0.0697 - RMSE: 0.2498 - val_loss: 0.1390 - val_RMSE: 0.3476
Epoch 10/20
13/13 [==============================] - 0s 11ms/step - loss: 0.0679 - RMSE: 0.2474 - val_loss: 0.1310 - val_RMSE: 0.3364
Epoch 11/20
13/13 [==============================] - 0s 11ms/step - loss: 0.0675 - RMSE: 0.2448 - val_loss: 0.1257 - val_RMSE: 0.3288
Epoch 12/20
13/13 [==============================] - 0s 10ms/step - loss: 0.0617 - RMSE: 0.2328 - val_loss: 0.1225 - val_RMSE: 0.3246
Epoch 13/20
13/13 [==============================] - 0s 10ms/step - loss: 0.0618 - RMSE: 0.2333 - val_loss: 0.1188 - val_RMSE: 0.3193
Epoch 14/20
13/13 [==============================] - 0s 10ms/step - loss: 0.0587 - RMSE: 0.2269 - val_loss: 0.1155 - val_RMSE: 0.3135
Epoch 15/20
13/13 [==============================] - 0s 12ms/step - loss: 0.0577 - RMSE: 0.2241 - val_loss: 0.1072 - val_RMSE: 0.3027
Epoch 16/20
13/13 [==============================] - 0s 11ms/step - loss: 0.0536 - RMSE: 0.2139 - val_loss: 0.1045 - val_RMSE: 0.2974
Epoch 17/20
13/13 [==============================] - 0s 10ms/step - loss: 0.0529 - RMSE: 0.2119 - val_loss: 0.1009 - val_RMSE: 0.2922
Epoch 18/20
13/13 [==============================] - 0s 10ms/step - loss: 0.0498 - RMSE: 0.2058 - val_loss: 0.0970 - val_RMSE: 0.2864
Epoch 19/20
13/13 [==============================] - 0s 11ms/step - loss: 0.0500 - RMSE: 0.2057 - val_loss: 0.0914 - val_RMSE: 0.2768
Epoch 20/20
13/13 [==============================] - 0s 10ms/step - loss: 0.0498 - RMSE: 0.2052 - val_loss: 0.0833 - val_RMSE: 0.2630

============================ Beginning trial with hyperparams: OPTIMIZER: SGD, LEARNING RATE: 0.05, EPOCHS: 40============================

Epoch 1/40
100/100 [==============================] - 1s 6ms/step - loss: 0.1762 - RMSE: 0.3769 - val_loss: 0.1566 - val_RMSE: 0.3789
Epoch 2/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0725 - RMSE: 0.2463 - val_loss: 0.0976 - val_RMSE: 0.2948
Epoch 3/40
100/100 [==============================] - 1s 6ms/step - loss: 0.0543 - RMSE: 0.2095 - val_loss: 0.0579 - val_RMSE: 0.2198
Epoch 4/40
100/100 [==============================] - 1s 6ms/step - loss: 0.0450 - RMSE: 0.1878 - val_loss: 0.0369 - val_RMSE: 0.1662
Epoch 5/40
100/100 [==============================] - 1s 6ms/step - loss: 0.0393 - RMSE: 0.1736 - val_loss: 0.0314 - val_RMSE: 0.1490
Epoch 6/40
100/100 [==============================] - 1s 7ms/step - loss: 0.0366 - RMSE: 0.1658 - val_loss: 0.0299 - val_RMSE: 0.1443
Epoch 7/40
100/100 [==============================] - 1s 6ms/step - loss: 0.0345 - RMSE: 0.1606 - val_loss: 0.0307 - val_RMSE: 0.1474
Epoch 8/40
100/100 [==============================] - 1s 6ms/step - loss: 0.0320 - RMSE: 0.1537 - val_loss: 0.0299 - val_RMSE: 0.1448
Epoch 9/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0308 - RMSE: 0.1502 - val_loss: 0.0316 - val_RMSE: 0.1500
Epoch 10/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0298 - RMSE: 0.1468 - val_loss: 0.0299 - val_RMSE: 0.1443
Epoch 11/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0291 - RMSE: 0.1451 - val_loss: 0.0271 - val_RMSE: 0.1357
Epoch 12/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0282 - RMSE: 0.1421 - val_loss: 0.0288 - val_RMSE: 0.1415
Epoch 13/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0276 - RMSE: 0.1392 - val_loss: 0.0283 - val_RMSE: 0.1398
Epoch 14/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0267 - RMSE: 0.1367 - val_loss: 0.0278 - val_RMSE: 0.1387
Epoch 15/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0268 - RMSE: 0.1378 - val_loss: 0.0293 - val_RMSE: 0.1433
Epoch 16/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0254 - RMSE: 0.1329 - val_loss: 0.0275 - val_RMSE: 0.1368
Epoch 17/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0256 - RMSE: 0.1325 - val_loss: 0.0277 - val_RMSE: 0.1378
Epoch 18/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0259 - RMSE: 0.1351 - val_loss: 0.0279 - val_RMSE: 0.1387
Epoch 19/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0250 - RMSE: 0.1318 - val_loss: 0.0268 - val_RMSE: 0.1343
Epoch 20/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0254 - RMSE: 0.1331 - val_loss: 0.0278 - val_RMSE: 0.1385
Epoch 21/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0253 - RMSE: 0.1331 - val_loss: 0.0264 - val_RMSE: 0.1335
Epoch 22/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0249 - RMSE: 0.1311 - val_loss: 0.0270 - val_RMSE: 0.1361
Epoch 23/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0251 - RMSE: 0.1321 - val_loss: 0.0275 - val_RMSE: 0.1381
Epoch 24/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0243 - RMSE: 0.1295 - val_loss: 0.0264 - val_RMSE: 0.1336
Epoch 25/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0241 - RMSE: 0.1290 - val_loss: 0.0264 - val_RMSE: 0.1342
Epoch 26/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0243 - RMSE: 0.1297 - val_loss: 0.0273 - val_RMSE: 0.1374
Epoch 27/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0244 - RMSE: 0.1302 - val_loss: 0.0263 - val_RMSE: 0.1341
Epoch 28/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0243 - RMSE: 0.1300 - val_loss: 0.0265 - val_RMSE: 0.1348
Epoch 29/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0244 - RMSE: 0.1301 - val_loss: 0.0265 - val_RMSE: 0.1347
Epoch 30/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0242 - RMSE: 0.1299 - val_loss: 0.0262 - val_RMSE: 0.1340
Epoch 31/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0234 - RMSE: 0.1262 - val_loss: 0.0263 - val_RMSE: 0.1347
Epoch 32/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0236 - RMSE: 0.1277 - val_loss: 0.0258 - val_RMSE: 0.1331
Epoch 33/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0239 - RMSE: 0.1286 - val_loss: 0.0269 - val_RMSE: 0.1370
Epoch 34/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0239 - RMSE: 0.1290 - val_loss: 0.0263 - val_RMSE: 0.1346
Epoch 35/40
100/100 [==============================] - 1s 6ms/step - loss: 0.0238 - RMSE: 0.1281 - val_loss: 0.0256 - val_RMSE: 0.1325
Epoch 36/40
100/100 [==============================] - 1s 6ms/step - loss: 0.0235 - RMSE: 0.1275 - val_loss: 0.0257 - val_RMSE: 0.1324
Epoch 37/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0232 - RMSE: 0.1267 - val_loss: 0.0260 - val_RMSE: 0.1336
Epoch 38/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0239 - RMSE: 0.1291 - val_loss: 0.0261 - val_RMSE: 0.1341
Epoch 39/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0234 - RMSE: 0.1276 - val_loss: 0.0269 - val_RMSE: 0.1372
Epoch 40/40
100/100 [==============================] - 1s 6ms/step - loss: 0.0241 - RMSE: 0.1302 - val_loss: 0.0266 - val_RMSE: 0.1365

============================ Beginning trial with hyperparams: OPTIMIZER: SGD, LEARNING RATE: 0.05, EPOCHS: 40============================

Epoch 1/40
25/25 [==============================] - 1s 11ms/step - loss: 0.3280 - RMSE: 0.5273 - val_loss: 0.2144 - val_RMSE: 0.4476
Epoch 2/40
25/25 [==============================] - 0s 8ms/step - loss: 0.1346 - RMSE: 0.3534 - val_loss: 0.2052 - val_RMSE: 0.4387
Epoch 3/40
25/25 [==============================] - 0s 8ms/step - loss: 0.1047 - RMSE: 0.3089 - val_loss: 0.1887 - val_RMSE: 0.4206
Epoch 4/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0883 - RMSE: 0.2820 - val_loss: 0.1708 - val_RMSE: 0.3993
Epoch 5/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0775 - RMSE: 0.2628 - val_loss: 0.1522 - val_RMSE: 0.3738
Epoch 6/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0717 - RMSE: 0.2516 - val_loss: 0.1432 - val_RMSE: 0.3641
Epoch 7/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0605 - RMSE: 0.2287 - val_loss: 0.1203 - val_RMSE: 0.3311
Epoch 8/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0577 - RMSE: 0.2223 - val_loss: 0.1136 - val_RMSE: 0.3216
Epoch 9/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0556 - RMSE: 0.2180 - val_loss: 0.1015 - val_RMSE: 0.3039
Epoch 10/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0548 - RMSE: 0.2157 - val_loss: 0.0834 - val_RMSE: 0.2724
Epoch 11/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0498 - RMSE: 0.2045 - val_loss: 0.0739 - val_RMSE: 0.2543
Epoch 12/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0483 - RMSE: 0.2017 - val_loss: 0.0693 - val_RMSE: 0.2450
Epoch 13/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0474 - RMSE: 0.1997 - val_loss: 0.0610 - val_RMSE: 0.2280
Epoch 14/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0432 - RMSE: 0.1884 - val_loss: 0.0570 - val_RMSE: 0.2193
Epoch 15/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0425 - RMSE: 0.1866 - val_loss: 0.0475 - val_RMSE: 0.1980
Epoch 16/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0431 - RMSE: 0.1886 - val_loss: 0.0457 - val_RMSE: 0.1938
Epoch 17/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0412 - RMSE: 0.1832 - val_loss: 0.0392 - val_RMSE: 0.1762
Epoch 18/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0404 - RMSE: 0.1810 - val_loss: 0.0350 - val_RMSE: 0.1646
Epoch 19/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0381 - RMSE: 0.1746 - val_loss: 0.0351 - val_RMSE: 0.1663
Epoch 20/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0361 - RMSE: 0.1692 - val_loss: 0.0309 - val_RMSE: 0.1519
Epoch 21/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0373 - RMSE: 0.1722 - val_loss: 0.0311 - val_RMSE: 0.1532
Epoch 22/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0369 - RMSE: 0.1717 - val_loss: 0.0296 - val_RMSE: 0.1482
Epoch 23/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0356 - RMSE: 0.1679 - val_loss: 0.0308 - val_RMSE: 0.1534
Epoch 24/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0348 - RMSE: 0.1657 - val_loss: 0.0284 - val_RMSE: 0.1441
Epoch 25/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0332 - RMSE: 0.1607 - val_loss: 0.0278 - val_RMSE: 0.1424
Epoch 26/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0326 - RMSE: 0.1591 - val_loss: 0.0285 - val_RMSE: 0.1468
Epoch 27/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0335 - RMSE: 0.1617 - val_loss: 0.0279 - val_RMSE: 0.1445
Epoch 28/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0325 - RMSE: 0.1586 - val_loss: 0.0279 - val_RMSE: 0.1438
Epoch 29/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0330 - RMSE: 0.1605 - val_loss: 0.0279 - val_RMSE: 0.1436
Epoch 30/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0311 - RMSE: 0.1547 - val_loss: 0.0275 - val_RMSE: 0.1413
Epoch 31/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0303 - RMSE: 0.1516 - val_loss: 0.0275 - val_RMSE: 0.1420
Epoch 32/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0310 - RMSE: 0.1538 - val_loss: 0.0274 - val_RMSE: 0.1420
Epoch 33/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0313 - RMSE: 0.1549 - val_loss: 0.0279 - val_RMSE: 0.1451
Epoch 34/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0308 - RMSE: 0.1536 - val_loss: 0.0282 - val_RMSE: 0.1471
Epoch 35/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0305 - RMSE: 0.1525 - val_loss: 0.0277 - val_RMSE: 0.1447
Epoch 36/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0301 - RMSE: 0.1514 - val_loss: 0.0273 - val_RMSE: 0.1428
Epoch 37/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0314 - RMSE: 0.1560 - val_loss: 0.0276 - val_RMSE: 0.1434
Epoch 38/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0291 - RMSE: 0.1476 - val_loss: 0.0278 - val_RMSE: 0.1440
Epoch 39/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0291 - RMSE: 0.1486 - val_loss: 0.0274 - val_RMSE: 0.1425
Epoch 40/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0281 - RMSE: 0.1449 - val_loss: 0.0274 - val_RMSE: 0.1430

============================ Beginning trial with hyperparams: OPTIMIZER: SGD, LEARNING RATE: 0.05, EPOCHS: 40============================

Epoch 1/40
13/13 [==============================] - 1s 21ms/step - loss: 0.4201 - RMSE: 0.5854 - val_loss: 0.3530 - val_RMSE: 0.5826
Epoch 2/40
13/13 [==============================] - 0s 8ms/step - loss: 0.1445 - RMSE: 0.3656 - val_loss: 0.3269 - val_RMSE: 0.5583
Epoch 3/40
13/13 [==============================] - 0s 9ms/step - loss: 0.1157 - RMSE: 0.3269 - val_loss: 0.3074 - val_RMSE: 0.5402
Epoch 4/40
13/13 [==============================] - 0s 10ms/step - loss: 0.1006 - RMSE: 0.3025 - val_loss: 0.2996 - val_RMSE: 0.5325
Epoch 5/40
13/13 [==============================] - 0s 10ms/step - loss: 0.0892 - RMSE: 0.2826 - val_loss: 0.2799 - val_RMSE: 0.5136
Epoch 6/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0768 - RMSE: 0.2614 - val_loss: 0.2699 - val_RMSE: 0.5049
Epoch 7/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0761 - RMSE: 0.2614 - val_loss: 0.2516 - val_RMSE: 0.4864
Epoch 8/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0733 - RMSE: 0.2534 - val_loss: 0.2477 - val_RMSE: 0.4823
Epoch 9/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0684 - RMSE: 0.2451 - val_loss: 0.2323 - val_RMSE: 0.4663
Epoch 10/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0634 - RMSE: 0.2355 - val_loss: 0.2239 - val_RMSE: 0.4577
Epoch 11/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0636 - RMSE: 0.2380 - val_loss: 0.2081 - val_RMSE: 0.4390
Epoch 12/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0593 - RMSE: 0.2260 - val_loss: 0.2005 - val_RMSE: 0.4311
Epoch 13/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0562 - RMSE: 0.2195 - val_loss: 0.1886 - val_RMSE: 0.4169
Epoch 14/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0555 - RMSE: 0.2175 - val_loss: 0.1788 - val_RMSE: 0.4053
Epoch 15/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0520 - RMSE: 0.2108 - val_loss: 0.1669 - val_RMSE: 0.3908
Epoch 16/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0515 - RMSE: 0.2093 - val_loss: 0.1532 - val_RMSE: 0.3731
Epoch 17/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0523 - RMSE: 0.2103 - val_loss: 0.1519 - val_RMSE: 0.3711
Epoch 18/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0477 - RMSE: 0.1994 - val_loss: 0.1426 - val_RMSE: 0.3589
Epoch 19/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0461 - RMSE: 0.1960 - val_loss: 0.1330 - val_RMSE: 0.3451
Epoch 20/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0476 - RMSE: 0.2008 - val_loss: 0.1235 - val_RMSE: 0.3320
Epoch 21/40
13/13 [==============================] - 0s 10ms/step - loss: 0.0447 - RMSE: 0.1918 - val_loss: 0.1172 - val_RMSE: 0.3214
Epoch 22/40
13/13 [==============================] - 0s 10ms/step - loss: 0.0485 - RMSE: 0.2018 - val_loss: 0.1078 - val_RMSE: 0.3063
Epoch 23/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0464 - RMSE: 0.1982 - val_loss: 0.1055 - val_RMSE: 0.3022
Epoch 24/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0431 - RMSE: 0.1882 - val_loss: 0.1013 - val_RMSE: 0.2965
Epoch 25/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0427 - RMSE: 0.1882 - val_loss: 0.0897 - val_RMSE: 0.2771
Epoch 26/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0408 - RMSE: 0.1830 - val_loss: 0.0827 - val_RMSE: 0.2649
Epoch 27/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0412 - RMSE: 0.1836 - val_loss: 0.0813 - val_RMSE: 0.2623
Epoch 28/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0400 - RMSE: 0.1814 - val_loss: 0.0683 - val_RMSE: 0.2374
Epoch 29/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0417 - RMSE: 0.1842 - val_loss: 0.0646 - val_RMSE: 0.2297
Epoch 30/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0389 - RMSE: 0.1782 - val_loss: 0.0631 - val_RMSE: 0.2266
Epoch 31/40
13/13 [==============================] - 0s 10ms/step - loss: 0.0389 - RMSE: 0.1779 - val_loss: 0.0594 - val_RMSE: 0.2196
Epoch 32/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0373 - RMSE: 0.1732 - val_loss: 0.0539 - val_RMSE: 0.2064
Epoch 33/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0381 - RMSE: 0.1744 - val_loss: 0.0520 - val_RMSE: 0.2013
Epoch 34/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0375 - RMSE: 0.1746 - val_loss: 0.0514 - val_RMSE: 0.2015
Epoch 35/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0376 - RMSE: 0.1737 - val_loss: 0.0467 - val_RMSE: 0.1903
Epoch 36/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0384 - RMSE: 0.1763 - val_loss: 0.0451 - val_RMSE: 0.1853
Epoch 37/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0378 - RMSE: 0.1752 - val_loss: 0.0396 - val_RMSE: 0.1709
Epoch 38/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0373 - RMSE: 0.1744 - val_loss: 0.0370 - val_RMSE: 0.1646
Epoch 39/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0346 - RMSE: 0.1664 - val_loss: 0.0380 - val_RMSE: 0.1670
Epoch 40/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0353 - RMSE: 0.1675 - val_loss: 0.0365 - val_RMSE: 0.1629

============================ Beginning trial with hyperparams: OPTIMIZER: SGD, LEARNING RATE: 0.001, EPOCHS: 20============================

Epoch 1/20
100/100 [==============================] - 1s 6ms/step - loss: 1.4925 - RMSE: 1.1455 - val_loss: 0.3627 - val_RMSE: 0.5890
Epoch 2/20
100/100 [==============================] - 1s 5ms/step - loss: 0.5972 - RMSE: 0.7433 - val_loss: 0.2449 - val_RMSE: 0.4801
Epoch 3/20
100/100 [==============================] - 1s 6ms/step - loss: 0.3635 - RMSE: 0.5809 - val_loss: 0.1737 - val_RMSE: 0.3992
Epoch 4/20
100/100 [==============================] - 1s 5ms/step - loss: 0.3070 - RMSE: 0.5302 - val_loss: 0.1333 - val_RMSE: 0.3448
Epoch 5/20
100/100 [==============================] - 0s 5ms/step - loss: 0.2500 - RMSE: 0.4750 - val_loss: 0.1282 - val_RMSE: 0.3360
Epoch 6/20
100/100 [==============================] - 1s 5ms/step - loss: 0.2343 - RMSE: 0.4597 - val_loss: 0.1357 - val_RMSE: 0.3449
Epoch 7/20
100/100 [==============================] - 1s 5ms/step - loss: 0.2152 - RMSE: 0.4428 - val_loss: 0.1361 - val_RMSE: 0.3441
Epoch 8/20
100/100 [==============================] - 1s 5ms/step - loss: 0.2057 - RMSE: 0.4316 - val_loss: 0.1342 - val_RMSE: 0.3408
Epoch 9/20
100/100 [==============================] - 1s 5ms/step - loss: 0.1965 - RMSE: 0.4197 - val_loss: 0.1312 - val_RMSE: 0.3356
Epoch 10/20
100/100 [==============================] - 1s 6ms/step - loss: 0.1770 - RMSE: 0.3958 - val_loss: 0.1234 - val_RMSE: 0.3247
Epoch 11/20
100/100 [==============================] - 1s 5ms/step - loss: 0.1708 - RMSE: 0.3901 - val_loss: 0.1131 - val_RMSE: 0.3101
Epoch 12/20
100/100 [==============================] - 1s 5ms/step - loss: 0.1683 - RMSE: 0.3877 - val_loss: 0.1076 - val_RMSE: 0.3022
Epoch 13/20
100/100 [==============================] - 1s 5ms/step - loss: 0.1663 - RMSE: 0.3846 - val_loss: 0.1008 - val_RMSE: 0.2918
Epoch 14/20
100/100 [==============================] - 1s 5ms/step - loss: 0.1543 - RMSE: 0.3713 - val_loss: 0.0964 - val_RMSE: 0.2851
Epoch 15/20
100/100 [==============================] - 1s 5ms/step - loss: 0.1537 - RMSE: 0.3690 - val_loss: 0.0935 - val_RMSE: 0.2801
Epoch 16/20
100/100 [==============================] - 1s 5ms/step - loss: 0.1508 - RMSE: 0.3647 - val_loss: 0.0921 - val_RMSE: 0.2779
Epoch 17/20
100/100 [==============================] - 1s 5ms/step - loss: 0.1470 - RMSE: 0.3615 - val_loss: 0.0879 - val_RMSE: 0.2710
Epoch 18/20
100/100 [==============================] - 1s 5ms/step - loss: 0.1377 - RMSE: 0.3486 - val_loss: 0.0838 - val_RMSE: 0.2642
Epoch 19/20
100/100 [==============================] - 1s 5ms/step - loss: 0.1395 - RMSE: 0.3513 - val_loss: 0.0833 - val_RMSE: 0.2631
Epoch 20/20
100/100 [==============================] - 1s 5ms/step - loss: 0.1316 - RMSE: 0.3396 - val_loss: 0.0793 - val_RMSE: 0.2566

============================ Beginning trial with hyperparams: OPTIMIZER: SGD, LEARNING RATE: 0.001, EPOCHS: 20============================

Epoch 1/20
25/25 [==============================] - 1s 12ms/step - loss: 2.1400 - RMSE: 1.4156 - val_loss: 0.4996 - val_RMSE: 0.6986
Epoch 2/20
25/25 [==============================] - 0s 6ms/step - loss: 1.4676 - RMSE: 1.1848 - val_loss: 0.4390 - val_RMSE: 0.6537
Epoch 3/20
25/25 [==============================] - 0s 7ms/step - loss: 1.1313 - RMSE: 1.0443 - val_loss: 0.3915 - val_RMSE: 0.6163
Epoch 4/20
25/25 [==============================] - 0s 7ms/step - loss: 0.8717 - RMSE: 0.9203 - val_loss: 0.3522 - val_RMSE: 0.5833
Epoch 5/20
25/25 [==============================] - 0s 7ms/step - loss: 0.7005 - RMSE: 0.8274 - val_loss: 0.3187 - val_RMSE: 0.5537
Epoch 6/20
25/25 [==============================] - 0s 7ms/step - loss: 0.5971 - RMSE: 0.7625 - val_loss: 0.2893 - val_RMSE: 0.5260
Epoch 7/20
25/25 [==============================] - 0s 7ms/step - loss: 0.5204 - RMSE: 0.7114 - val_loss: 0.2626 - val_RMSE: 0.4999
Epoch 8/20
25/25 [==============================] - 0s 7ms/step - loss: 0.4450 - RMSE: 0.6568 - val_loss: 0.2408 - val_RMSE: 0.4774
Epoch 9/20
25/25 [==============================] - 0s 7ms/step - loss: 0.4185 - RMSE: 0.6369 - val_loss: 0.2211 - val_RMSE: 0.4563
Epoch 10/20
25/25 [==============================] - 0s 8ms/step - loss: 0.3834 - RMSE: 0.6074 - val_loss: 0.2028 - val_RMSE: 0.4357
Epoch 11/20
25/25 [==============================] - 0s 3ms/step - loss: 0.3409 - RMSE: 0.5725 - val_loss: 0.1864 - val_RMSE: 0.4166
Epoch 12/20
25/25 [==============================] - 0s 3ms/step - loss: 0.3116 - RMSE: 0.5471 - val_loss: 0.1729 - val_RMSE: 0.4001
Epoch 13/20
25/25 [==============================] - 0s 7ms/step - loss: 0.2913 - RMSE: 0.5296 - val_loss: 0.1604 - val_RMSE: 0.3841
Epoch 14/20
25/25 [==============================] - 0s 7ms/step - loss: 0.2958 - RMSE: 0.5329 - val_loss: 0.1492 - val_RMSE: 0.3692
Epoch 15/20
25/25 [==============================] - 0s 4ms/step - loss: 0.2664 - RMSE: 0.5041 - val_loss: 0.1397 - val_RMSE: 0.3561
Epoch 16/20
25/25 [==============================] - 0s 3ms/step - loss: 0.2587 - RMSE: 0.4965 - val_loss: 0.1333 - val_RMSE: 0.3471
Epoch 17/20
25/25 [==============================] - 0s 3ms/step - loss: 0.2640 - RMSE: 0.5026 - val_loss: 0.1280 - val_RMSE: 0.3392
Epoch 18/20
25/25 [==============================] - 0s 3ms/step - loss: 0.2366 - RMSE: 0.4748 - val_loss: 0.1245 - val_RMSE: 0.3339
Epoch 19/20
25/25 [==============================] - 0s 3ms/step - loss: 0.2422 - RMSE: 0.4796 - val_loss: 0.1217 - val_RMSE: 0.3298
Epoch 20/20
25/25 [==============================] - 0s 3ms/step - loss: 0.2193 - RMSE: 0.4575 - val_loss: 0.1196 - val_RMSE: 0.3266

============================ Beginning trial with hyperparams: OPTIMIZER: SGD, LEARNING RATE: 0.001, EPOCHS: 20============================

Epoch 1/20
13/13 [==============================] - 1s 21ms/step - loss: 1.1710 - RMSE: 1.0664 - val_loss: 0.5295 - val_RMSE: 0.7297
Epoch 2/20
13/13 [==============================] - 0s 8ms/step - loss: 1.0573 - RMSE: 1.0109 - val_loss: 0.5015 - val_RMSE: 0.7101
Epoch 3/20
13/13 [==============================] - 0s 9ms/step - loss: 0.9245 - RMSE: 0.9460 - val_loss: 0.4763 - val_RMSE: 0.6919
Epoch 4/20
13/13 [==============================] - 0s 10ms/step - loss: 0.8467 - RMSE: 0.9079 - val_loss: 0.4535 - val_RMSE: 0.6752
Epoch 5/20
13/13 [==============================] - 0s 10ms/step - loss: 0.7552 - RMSE: 0.8557 - val_loss: 0.4328 - val_RMSE: 0.6598
Epoch 6/20
13/13 [==============================] - 0s 13ms/step - loss: 0.6754 - RMSE: 0.8041 - val_loss: 0.4134 - val_RMSE: 0.6452
Epoch 7/20
13/13 [==============================] - 0s 9ms/step - loss: 0.6221 - RMSE: 0.7772 - val_loss: 0.3943 - val_RMSE: 0.6302
Epoch 8/20
13/13 [==============================] - 0s 5ms/step - loss: 0.5819 - RMSE: 0.7523 - val_loss: 0.3763 - val_RMSE: 0.6157
Epoch 9/20
13/13 [==============================] - 0s 5ms/step - loss: 0.5322 - RMSE: 0.7209 - val_loss: 0.3601 - val_RMSE: 0.6024
Epoch 10/20
13/13 [==============================] - 0s 8ms/step - loss: 0.5054 - RMSE: 0.6974 - val_loss: 0.3451 - val_RMSE: 0.5898
Epoch 11/20
13/13 [==============================] - 0s 11ms/step - loss: 0.4651 - RMSE: 0.6727 - val_loss: 0.3302 - val_RMSE: 0.5771
Epoch 12/20
13/13 [==============================] - 0s 10ms/step - loss: 0.4348 - RMSE: 0.6505 - val_loss: 0.3159 - val_RMSE: 0.5646
Epoch 13/20
13/13 [==============================] - 0s 10ms/step - loss: 0.3964 - RMSE: 0.6198 - val_loss: 0.3021 - val_RMSE: 0.5522
Epoch 14/20
13/13 [==============================] - 0s 10ms/step - loss: 0.3973 - RMSE: 0.6167 - val_loss: 0.2891 - val_RMSE: 0.5402
Epoch 15/20
13/13 [==============================] - 0s 11ms/step - loss: 0.3757 - RMSE: 0.5998 - val_loss: 0.2761 - val_RMSE: 0.5280
Epoch 16/20
13/13 [==============================] - 0s 11ms/step - loss: 0.3562 - RMSE: 0.5852 - val_loss: 0.2641 - val_RMSE: 0.5165
Epoch 17/20
13/13 [==============================] - 0s 10ms/step - loss: 0.3492 - RMSE: 0.5786 - val_loss: 0.2525 - val_RMSE: 0.5050
Epoch 18/20
13/13 [==============================] - 0s 9ms/step - loss: 0.3220 - RMSE: 0.5543 - val_loss: 0.2419 - val_RMSE: 0.4943
Epoch 19/20
13/13 [==============================] - 0s 10ms/step - loss: 0.3011 - RMSE: 0.5371 - val_loss: 0.2317 - val_RMSE: 0.4838
Epoch 20/20
13/13 [==============================] - 0s 10ms/step - loss: 0.2996 - RMSE: 0.5360 - val_loss: 0.2220 - val_RMSE: 0.4735

============================ Beginning trial with hyperparams: OPTIMIZER: SGD, LEARNING RATE: 0.001, EPOCHS: 40============================

Epoch 1/40
100/100 [==============================] - 1s 5ms/step - loss: 1.1344 - RMSE: 1.0284 - val_loss: 0.3520 - val_RMSE: 0.5820
Epoch 2/40
100/100 [==============================] - 0s 5ms/step - loss: 0.5744 - RMSE: 0.7307 - val_loss: 0.2511 - val_RMSE: 0.4874
Epoch 3/40
100/100 [==============================] - 0s 4ms/step - loss: 0.3832 - RMSE: 0.5928 - val_loss: 0.1902 - val_RMSE: 0.4187
Epoch 4/40
100/100 [==============================] - 1s 7ms/step - loss: 0.3131 - RMSE: 0.5327 - val_loss: 0.1658 - val_RMSE: 0.3824
Epoch 5/40
100/100 [==============================] - 0s 5ms/step - loss: 0.2603 - RMSE: 0.4851 - val_loss: 0.1628 - val_RMSE: 0.3740
Epoch 6/40
100/100 [==============================] - 0s 4ms/step - loss: 0.2194 - RMSE: 0.4442 - val_loss: 0.1653 - val_RMSE: 0.3774
Epoch 7/40
100/100 [==============================] - 0s 3ms/step - loss: 0.2145 - RMSE: 0.4385 - val_loss: 0.1656 - val_RMSE: 0.3788
Epoch 8/40
100/100 [==============================] - 0s 4ms/step - loss: 0.1976 - RMSE: 0.4220 - val_loss: 0.1582 - val_RMSE: 0.3704
Epoch 9/40
100/100 [==============================] - 0s 5ms/step - loss: 0.1781 - RMSE: 0.3989 - val_loss: 0.1476 - val_RMSE: 0.3574
Epoch 10/40
100/100 [==============================] - 0s 2ms/step - loss: 0.1791 - RMSE: 0.4007 - val_loss: 0.1398 - val_RMSE: 0.3475
Epoch 11/40
100/100 [==============================] - 0s 4ms/step - loss: 0.1642 - RMSE: 0.3820 - val_loss: 0.1315 - val_RMSE: 0.3366
Epoch 12/40
100/100 [==============================] - 0s 4ms/step - loss: 0.1606 - RMSE: 0.3793 - val_loss: 0.1227 - val_RMSE: 0.3248
Epoch 13/40
100/100 [==============================] - 1s 7ms/step - loss: 0.1565 - RMSE: 0.3735 - val_loss: 0.1172 - val_RMSE: 0.3174
Epoch 14/40
100/100 [==============================] - 1s 6ms/step - loss: 0.1487 - RMSE: 0.3630 - val_loss: 0.1107 - val_RMSE: 0.3082
Epoch 15/40
100/100 [==============================] - 0s 2ms/step - loss: 0.1378 - RMSE: 0.3495 - val_loss: 0.1080 - val_RMSE: 0.3041
Epoch 16/40
100/100 [==============================] - 0s 4ms/step - loss: 0.1349 - RMSE: 0.3449 - val_loss: 0.1022 - val_RMSE: 0.2949
Epoch 17/40
100/100 [==============================] - 0s 3ms/step - loss: 0.1349 - RMSE: 0.3442 - val_loss: 0.0979 - val_RMSE: 0.2884
Epoch 18/40
100/100 [==============================] - 0s 3ms/step - loss: 0.1324 - RMSE: 0.3404 - val_loss: 0.0937 - val_RMSE: 0.2816
Epoch 19/40
100/100 [==============================] - 0s 2ms/step - loss: 0.1274 - RMSE: 0.3350 - val_loss: 0.0900 - val_RMSE: 0.2754
Epoch 20/40
100/100 [==============================] - 0s 3ms/step - loss: 0.1266 - RMSE: 0.3343 - val_loss: 0.0882 - val_RMSE: 0.2728
Epoch 21/40
100/100 [==============================] - 0s 2ms/step - loss: 0.1214 - RMSE: 0.3253 - val_loss: 0.0854 - val_RMSE: 0.2681
Epoch 22/40
100/100 [==============================] - 0s 2ms/step - loss: 0.1261 - RMSE: 0.3312 - val_loss: 0.0818 - val_RMSE: 0.2621
Epoch 23/40
100/100 [==============================] - 0s 2ms/step - loss: 0.1229 - RMSE: 0.3289 - val_loss: 0.0804 - val_RMSE: 0.2597
Epoch 24/40
100/100 [==============================] - 0s 2ms/step - loss: 0.1122 - RMSE: 0.3129 - val_loss: 0.0783 - val_RMSE: 0.2558
Epoch 25/40
100/100 [==============================] - 0s 2ms/step - loss: 0.1102 - RMSE: 0.3097 - val_loss: 0.0771 - val_RMSE: 0.2533
Epoch 26/40
100/100 [==============================] - 0s 2ms/step - loss: 0.1149 - RMSE: 0.3177 - val_loss: 0.0743 - val_RMSE: 0.2482
Epoch 27/40
100/100 [==============================] - 0s 2ms/step - loss: 0.1094 - RMSE: 0.3073 - val_loss: 0.0745 - val_RMSE: 0.2490
Epoch 28/40
100/100 [==============================] - 0s 2ms/step - loss: 0.1072 - RMSE: 0.3040 - val_loss: 0.0719 - val_RMSE: 0.2440
Epoch 29/40
100/100 [==============================] - 0s 2ms/step - loss: 0.1065 - RMSE: 0.3014 - val_loss: 0.0721 - val_RMSE: 0.2445
Epoch 30/40
100/100 [==============================] - 0s 2ms/step - loss: 0.1053 - RMSE: 0.3022 - val_loss: 0.0699 - val_RMSE: 0.2404
Epoch 31/40
100/100 [==============================] - 0s 3ms/step - loss: 0.1024 - RMSE: 0.2973 - val_loss: 0.0691 - val_RMSE: 0.2393
Epoch 32/40
100/100 [==============================] - 1s 6ms/step - loss: 0.0977 - RMSE: 0.2910 - val_loss: 0.0685 - val_RMSE: 0.2382
Epoch 33/40
100/100 [==============================] - 1s 6ms/step - loss: 0.0994 - RMSE: 0.2924 - val_loss: 0.0671 - val_RMSE: 0.2354
Epoch 34/40
100/100 [==============================] - 0s 5ms/step - loss: 0.0959 - RMSE: 0.2861 - val_loss: 0.0654 - val_RMSE: 0.2324
Epoch 35/40
100/100 [==============================] - 0s 5ms/step - loss: 0.1018 - RMSE: 0.2970 - val_loss: 0.0643 - val_RMSE: 0.2299
Epoch 36/40
100/100 [==============================] - 0s 5ms/step - loss: 0.0941 - RMSE: 0.2847 - val_loss: 0.0646 - val_RMSE: 0.2304
Epoch 37/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0947 - RMSE: 0.2849 - val_loss: 0.0640 - val_RMSE: 0.2294
Epoch 38/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0958 - RMSE: 0.2880 - val_loss: 0.0629 - val_RMSE: 0.2272
Epoch 39/40
100/100 [==============================] - 1s 6ms/step - loss: 0.0910 - RMSE: 0.2810 - val_loss: 0.0622 - val_RMSE: 0.2258
Epoch 40/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0951 - RMSE: 0.2861 - val_loss: 0.0609 - val_RMSE: 0.2230

============================ Beginning trial with hyperparams: OPTIMIZER: SGD, LEARNING RATE: 0.001, EPOCHS: 40============================

Epoch 1/40
25/25 [==============================] - 1s 13ms/step - loss: 1.0416 - RMSE: 0.9979 - val_loss: 0.4189 - val_RMSE: 0.6151
Epoch 2/40
25/25 [==============================] - 0s 8ms/step - loss: 0.8462 - RMSE: 0.9014 - val_loss: 0.3783 - val_RMSE: 0.5914
Epoch 3/40
25/25 [==============================] - 0s 5ms/step - loss: 0.6612 - RMSE: 0.7996 - val_loss: 0.3473 - val_RMSE: 0.5702
Epoch 4/40
25/25 [==============================] - 0s 3ms/step - loss: 0.5780 - RMSE: 0.7473 - val_loss: 0.3214 - val_RMSE: 0.5507
Epoch 5/40
25/25 [==============================] - 0s 3ms/step - loss: 0.5041 - RMSE: 0.6998 - val_loss: 0.2988 - val_RMSE: 0.5321
Epoch 6/40
25/25 [==============================] - 0s 3ms/step - loss: 0.4495 - RMSE: 0.6607 - val_loss: 0.2783 - val_RMSE: 0.5138
Epoch 7/40
25/25 [==============================] - 0s 7ms/step - loss: 0.3955 - RMSE: 0.6180 - val_loss: 0.2587 - val_RMSE: 0.4952
Epoch 8/40
25/25 [==============================] - 0s 8ms/step - loss: 0.3656 - RMSE: 0.5945 - val_loss: 0.2401 - val_RMSE: 0.4766
Epoch 9/40
25/25 [==============================] - 0s 8ms/step - loss: 0.3362 - RMSE: 0.5698 - val_loss: 0.2233 - val_RMSE: 0.4589
Epoch 10/40
25/25 [==============================] - 0s 8ms/step - loss: 0.3197 - RMSE: 0.5555 - val_loss: 0.2066 - val_RMSE: 0.4405
Epoch 11/40
25/25 [==============================] - 0s 8ms/step - loss: 0.2911 - RMSE: 0.5289 - val_loss: 0.1919 - val_RMSE: 0.4234
Epoch 12/40
25/25 [==============================] - 0s 3ms/step - loss: 0.2871 - RMSE: 0.5249 - val_loss: 0.1780 - val_RMSE: 0.4064
Epoch 13/40
25/25 [==============================] - 0s 5ms/step - loss: 0.2734 - RMSE: 0.5114 - val_loss: 0.1657 - val_RMSE: 0.3908
Epoch 14/40
25/25 [==============================] - 0s 9ms/step - loss: 0.2540 - RMSE: 0.4932 - val_loss: 0.1549 - val_RMSE: 0.3765
Epoch 15/40
25/25 [==============================] - 0s 6ms/step - loss: 0.2459 - RMSE: 0.4845 - val_loss: 0.1460 - val_RMSE: 0.3642
Epoch 16/40
25/25 [==============================] - 0s 8ms/step - loss: 0.2509 - RMSE: 0.4895 - val_loss: 0.1381 - val_RMSE: 0.3531
Epoch 17/40
25/25 [==============================] - 0s 4ms/step - loss: 0.2345 - RMSE: 0.4729 - val_loss: 0.1326 - val_RMSE: 0.3453
Epoch 18/40
25/25 [==============================] - 0s 4ms/step - loss: 0.2297 - RMSE: 0.4674 - val_loss: 0.1284 - val_RMSE: 0.3395
Epoch 19/40
25/25 [==============================] - 0s 9ms/step - loss: 0.2211 - RMSE: 0.4582 - val_loss: 0.1249 - val_RMSE: 0.3347
Epoch 20/40
25/25 [==============================] - 0s 5ms/step - loss: 0.2177 - RMSE: 0.4544 - val_loss: 0.1229 - val_RMSE: 0.3320
Epoch 21/40
25/25 [==============================] - 0s 6ms/step - loss: 0.2080 - RMSE: 0.4446 - val_loss: 0.1208 - val_RMSE: 0.3293
Epoch 22/40
25/25 [==============================] - 0s 10ms/step - loss: 0.1972 - RMSE: 0.4326 - val_loss: 0.1195 - val_RMSE: 0.3275
Epoch 23/40
25/25 [==============================] - 0s 8ms/step - loss: 0.2124 - RMSE: 0.4496 - val_loss: 0.1182 - val_RMSE: 0.3259
Epoch 24/40
25/25 [==============================] - 0s 10ms/step - loss: 0.1892 - RMSE: 0.4236 - val_loss: 0.1171 - val_RMSE: 0.3243
Epoch 25/40
25/25 [==============================] - 0s 8ms/step - loss: 0.1924 - RMSE: 0.4268 - val_loss: 0.1164 - val_RMSE: 0.3230
Epoch 26/40
25/25 [==============================] - 0s 9ms/step - loss: 0.1807 - RMSE: 0.4134 - val_loss: 0.1150 - val_RMSE: 0.3208
Epoch 27/40
25/25 [==============================] - 0s 8ms/step - loss: 0.1853 - RMSE: 0.4179 - val_loss: 0.1138 - val_RMSE: 0.3187
Epoch 28/40
25/25 [==============================] - 0s 6ms/step - loss: 0.1797 - RMSE: 0.4118 - val_loss: 0.1126 - val_RMSE: 0.3170
Epoch 29/40
25/25 [==============================] - 0s 8ms/step - loss: 0.1805 - RMSE: 0.4122 - val_loss: 0.1113 - val_RMSE: 0.3149
Epoch 30/40
25/25 [==============================] - 0s 6ms/step - loss: 0.1807 - RMSE: 0.4133 - val_loss: 0.1097 - val_RMSE: 0.3122
Epoch 31/40
25/25 [==============================] - 0s 10ms/step - loss: 0.1769 - RMSE: 0.4089 - val_loss: 0.1082 - val_RMSE: 0.3097
Epoch 32/40
25/25 [==============================] - 0s 4ms/step - loss: 0.1657 - RMSE: 0.3948 - val_loss: 0.1067 - val_RMSE: 0.3074
Epoch 33/40
25/25 [==============================] - 0s 4ms/step - loss: 0.1644 - RMSE: 0.3925 - val_loss: 0.1052 - val_RMSE: 0.3048
Epoch 34/40
25/25 [==============================] - 0s 5ms/step - loss: 0.1683 - RMSE: 0.3977 - val_loss: 0.1032 - val_RMSE: 0.3013
Epoch 35/40
25/25 [==============================] - 0s 6ms/step - loss: 0.1651 - RMSE: 0.3947 - val_loss: 0.1014 - val_RMSE: 0.2981
Epoch 36/40
25/25 [==============================] - 0s 9ms/step - loss: 0.1650 - RMSE: 0.3930 - val_loss: 0.0995 - val_RMSE: 0.2951
Epoch 37/40
25/25 [==============================] - 0s 9ms/step - loss: 0.1572 - RMSE: 0.3846 - val_loss: 0.0980 - val_RMSE: 0.2926
Epoch 38/40
25/25 [==============================] - 0s 8ms/step - loss: 0.1598 - RMSE: 0.3867 - val_loss: 0.0966 - val_RMSE: 0.2905
Epoch 39/40
25/25 [==============================] - 0s 10ms/step - loss: 0.1553 - RMSE: 0.3810 - val_loss: 0.0948 - val_RMSE: 0.2874
Epoch 40/40
25/25 [==============================] - 0s 9ms/step - loss: 0.1589 - RMSE: 0.3863 - val_loss: 0.0931 - val_RMSE: 0.2846

============================ Beginning trial with hyperparams: OPTIMIZER: SGD, LEARNING RATE: 0.001, EPOCHS: 40============================

Epoch 1/40
13/13 [==============================] - 1s 25ms/step - loss: 1.7781 - RMSE: 1.3000 - val_loss: 0.5638 - val_RMSE: 0.7362
Epoch 2/40
13/13 [==============================] - 0s 16ms/step - loss: 1.5680 - RMSE: 1.2223 - val_loss: 0.5283 - val_RMSE: 0.7148
Epoch 3/40
13/13 [==============================] - 0s 11ms/step - loss: 1.2943 - RMSE: 1.1138 - val_loss: 0.4986 - val_RMSE: 0.6961
Epoch 4/40
13/13 [==============================] - 0s 4ms/step - loss: 1.1446 - RMSE: 1.0415 - val_loss: 0.4707 - val_RMSE: 0.6776
Epoch 5/40
13/13 [==============================] - 0s 5ms/step - loss: 1.0067 - RMSE: 0.9769 - val_loss: 0.4461 - val_RMSE: 0.6607
Epoch 6/40
13/13 [==============================] - 0s 4ms/step - loss: 0.9021 - RMSE: 0.9277 - val_loss: 0.4233 - val_RMSE: 0.6445
Epoch 7/40
13/13 [==============================] - 0s 5ms/step - loss: 0.8046 - RMSE: 0.8777 - val_loss: 0.4024 - val_RMSE: 0.6292
Epoch 8/40
13/13 [==============================] - 0s 4ms/step - loss: 0.7488 - RMSE: 0.8459 - val_loss: 0.3825 - val_RMSE: 0.6140
Epoch 9/40
13/13 [==============================] - 0s 4ms/step - loss: 0.6568 - RMSE: 0.7921 - val_loss: 0.3639 - val_RMSE: 0.5995
Epoch 10/40
13/13 [==============================] - 0s 4ms/step - loss: 0.5893 - RMSE: 0.7537 - val_loss: 0.3470 - val_RMSE: 0.5858
Epoch 11/40
13/13 [==============================] - 0s 7ms/step - loss: 0.5552 - RMSE: 0.7328 - val_loss: 0.3298 - val_RMSE: 0.5716
Epoch 12/40
13/13 [==============================] - 0s 5ms/step - loss: 0.5087 - RMSE: 0.7001 - val_loss: 0.3139 - val_RMSE: 0.5580
Epoch 13/40
13/13 [==============================] - 0s 6ms/step - loss: 0.4911 - RMSE: 0.6859 - val_loss: 0.2984 - val_RMSE: 0.5444
Epoch 14/40
13/13 [==============================] - 0s 4ms/step - loss: 0.4373 - RMSE: 0.6493 - val_loss: 0.2847 - val_RMSE: 0.5319
Epoch 15/40
13/13 [==============================] - 0s 5ms/step - loss: 0.4152 - RMSE: 0.6328 - val_loss: 0.2713 - val_RMSE: 0.5193
Epoch 16/40
13/13 [==============================] - 0s 4ms/step - loss: 0.3911 - RMSE: 0.6156 - val_loss: 0.2584 - val_RMSE: 0.5068
Epoch 17/40
13/13 [==============================] - 0s 6ms/step - loss: 0.3703 - RMSE: 0.5975 - val_loss: 0.2460 - val_RMSE: 0.4946
Epoch 18/40
13/13 [==============================] - 0s 7ms/step - loss: 0.3515 - RMSE: 0.5802 - val_loss: 0.2346 - val_RMSE: 0.4829
Epoch 19/40
13/13 [==============================] - 0s 11ms/step - loss: 0.3275 - RMSE: 0.5608 - val_loss: 0.2234 - val_RMSE: 0.4710
Epoch 20/40
13/13 [==============================] - 0s 12ms/step - loss: 0.3196 - RMSE: 0.5552 - val_loss: 0.2138 - val_RMSE: 0.4607
Epoch 21/40
13/13 [==============================] - 0s 9ms/step - loss: 0.3018 - RMSE: 0.5372 - val_loss: 0.2043 - val_RMSE: 0.4499
Epoch 22/40
13/13 [==============================] - 0s 4ms/step - loss: 0.2788 - RMSE: 0.5167 - val_loss: 0.1954 - val_RMSE: 0.4397
Epoch 23/40
13/13 [==============================] - 0s 4ms/step - loss: 0.2862 - RMSE: 0.5247 - val_loss: 0.1869 - val_RMSE: 0.4298
Epoch 24/40
13/13 [==============================] - 0s 6ms/step - loss: 0.2781 - RMSE: 0.5155 - val_loss: 0.1787 - val_RMSE: 0.4197
Epoch 25/40
13/13 [==============================] - 0s 7ms/step - loss: 0.2655 - RMSE: 0.5053 - val_loss: 0.1715 - val_RMSE: 0.4106
Epoch 26/40
13/13 [==============================] - 0s 12ms/step - loss: 0.2571 - RMSE: 0.4936 - val_loss: 0.1651 - val_RMSE: 0.4024
Epoch 27/40
13/13 [==============================] - 0s 8ms/step - loss: 0.2449 - RMSE: 0.4862 - val_loss: 0.1591 - val_RMSE: 0.3945
Epoch 28/40
13/13 [==============================] - 0s 4ms/step - loss: 0.2415 - RMSE: 0.4778 - val_loss: 0.1536 - val_RMSE: 0.3868
Epoch 29/40
13/13 [==============================] - 0s 4ms/step - loss: 0.2357 - RMSE: 0.4762 - val_loss: 0.1487 - val_RMSE: 0.3798
Epoch 30/40
13/13 [==============================] - 0s 5ms/step - loss: 0.2434 - RMSE: 0.4824 - val_loss: 0.1439 - val_RMSE: 0.3730
Epoch 31/40
13/13 [==============================] - 0s 6ms/step - loss: 0.2336 - RMSE: 0.4750 - val_loss: 0.1399 - val_RMSE: 0.3669
Epoch 32/40
13/13 [==============================] - 0s 6ms/step - loss: 0.2235 - RMSE: 0.4609 - val_loss: 0.1360 - val_RMSE: 0.3607
Epoch 33/40
13/13 [==============================] - 0s 6ms/step - loss: 0.2227 - RMSE: 0.4619 - val_loss: 0.1327 - val_RMSE: 0.3556
Epoch 34/40
13/13 [==============================] - 0s 9ms/step - loss: 0.2111 - RMSE: 0.4492 - val_loss: 0.1297 - val_RMSE: 0.3505
Epoch 35/40
13/13 [==============================] - 0s 12ms/step - loss: 0.2124 - RMSE: 0.4491 - val_loss: 0.1275 - val_RMSE: 0.3464
Epoch 36/40
13/13 [==============================] - 0s 7ms/step - loss: 0.2071 - RMSE: 0.4453 - val_loss: 0.1256 - val_RMSE: 0.3429
Epoch 37/40
13/13 [==============================] - 0s 7ms/step - loss: 0.2051 - RMSE: 0.4409 - val_loss: 0.1237 - val_RMSE: 0.3392
Epoch 38/40
13/13 [==============================] - 0s 5ms/step - loss: 0.2095 - RMSE: 0.4469 - val_loss: 0.1222 - val_RMSE: 0.3362
Epoch 39/40
13/13 [==============================] - 0s 4ms/step - loss: 0.1967 - RMSE: 0.4324 - val_loss: 0.1208 - val_RMSE: 0.3331
Epoch 40/40
13/13 [==============================] - 0s 4ms/step - loss: 0.1983 - RMSE: 0.4315 - val_loss: 0.1198 - val_RMSE: 0.3307

============================ Beginning trial with hyperparams: OPTIMIZER: Adam, LEARNING RATE: 0.05, EPOCHS: 20============================

Epoch 1/20
100/100 [==============================] - 1s 6ms/step - loss: 0.1451 - RMSE: 0.2633 - val_loss: 0.0364 - val_RMSE: 0.1476
Epoch 2/20
100/100 [==============================] - 0s 4ms/step - loss: 0.0333 - RMSE: 0.1411 - val_loss: 0.0336 - val_RMSE: 0.1428
Epoch 3/20
100/100 [==============================] - 0s 4ms/step - loss: 0.0318 - RMSE: 0.1396 - val_loss: 0.0324 - val_RMSE: 0.1425
Epoch 4/20
100/100 [==============================] - 0s 5ms/step - loss: 0.0297 - RMSE: 0.1371 - val_loss: 0.0307 - val_RMSE: 0.1407
Epoch 5/20
100/100 [==============================] - 0s 5ms/step - loss: 0.0285 - RMSE: 0.1382 - val_loss: 0.0292 - val_RMSE: 0.1410
Epoch 6/20
100/100 [==============================] - 0s 3ms/step - loss: 0.0284 - RMSE: 0.1417 - val_loss: 0.0300 - val_RMSE: 0.1471
Epoch 7/20
100/100 [==============================] - 0s 4ms/step - loss: 0.0265 - RMSE: 0.1396 - val_loss: 0.0364 - val_RMSE: 0.1706
Epoch 8/20
100/100 [==============================] - 0s 5ms/step - loss: 0.0261 - RMSE: 0.1417 - val_loss: 0.0378 - val_RMSE: 0.1780
Epoch 9/20
100/100 [==============================] - 0s 5ms/step - loss: 0.0253 - RMSE: 0.1421 - val_loss: 0.0260 - val_RMSE: 0.1435
Epoch 10/20
100/100 [==============================] - 1s 5ms/step - loss: 0.0240 - RMSE: 0.1402 - val_loss: 0.0233 - val_RMSE: 0.1375
Epoch 11/20
100/100 [==============================] - 0s 4ms/step - loss: 0.0240 - RMSE: 0.1421 - val_loss: 0.0242 - val_RMSE: 0.1421
Epoch 12/20
100/100 [==============================] - 0s 5ms/step - loss: 0.0221 - RMSE: 0.1366 - val_loss: 0.0235 - val_RMSE: 0.1412
Epoch 13/20
100/100 [==============================] - 0s 4ms/step - loss: 0.0238 - RMSE: 0.1441 - val_loss: 0.0221 - val_RMSE: 0.1371
Epoch 14/20
100/100 [==============================] - 1s 6ms/step - loss: 0.0234 - RMSE: 0.1423 - val_loss: 0.0248 - val_RMSE: 0.1472
Epoch 15/20
100/100 [==============================] - 1s 6ms/step - loss: 0.0222 - RMSE: 0.1403 - val_loss: 0.0247 - val_RMSE: 0.1462
Epoch 16/20
100/100 [==============================] - 0s 4ms/step - loss: 0.0224 - RMSE: 0.1414 - val_loss: 0.0250 - val_RMSE: 0.1480
Epoch 17/20
100/100 [==============================] - 0s 4ms/step - loss: 0.0233 - RMSE: 0.1441 - val_loss: 0.0256 - val_RMSE: 0.1482
Epoch 18/20
100/100 [==============================] - 0s 5ms/step - loss: 0.0237 - RMSE: 0.1451 - val_loss: 0.0269 - val_RMSE: 0.1532
Epoch 19/20
100/100 [==============================] - 1s 6ms/step - loss: 0.0233 - RMSE: 0.1436 - val_loss: 0.0225 - val_RMSE: 0.1401
Epoch 20/20
100/100 [==============================] - 1s 6ms/step - loss: 0.0220 - RMSE: 0.1405 - val_loss: 0.0354 - val_RMSE: 0.1740

============================ Beginning trial with hyperparams: OPTIMIZER: Adam, LEARNING RATE: 0.05, EPOCHS: 20============================

Epoch 1/20
25/25 [==============================] - 1s 12ms/step - loss: 0.2840 - RMSE: 0.4013 - val_loss: 0.0520 - val_RMSE: 0.1926
Epoch 2/20
25/25 [==============================] - 0s 8ms/step - loss: 0.0362 - RMSE: 0.1528 - val_loss: 0.0383 - val_RMSE: 0.1557
Epoch 3/20
25/25 [==============================] - 0s 6ms/step - loss: 0.0328 - RMSE: 0.1416 - val_loss: 0.0357 - val_RMSE: 0.1475
Epoch 4/20
25/25 [==============================] - 0s 7ms/step - loss: 0.0308 - RMSE: 0.1363 - val_loss: 0.0324 - val_RMSE: 0.1406
Epoch 5/20
25/25 [==============================] - 0s 8ms/step - loss: 0.0311 - RMSE: 0.1387 - val_loss: 0.0349 - val_RMSE: 0.1498
Epoch 6/20
25/25 [==============================] - 0s 9ms/step - loss: 0.0298 - RMSE: 0.1367 - val_loss: 0.0310 - val_RMSE: 0.1380
Epoch 7/20
25/25 [==============================] - 0s 4ms/step - loss: 0.0280 - RMSE: 0.1323 - val_loss: 0.0306 - val_RMSE: 0.1380
Epoch 8/20
25/25 [==============================] - 0s 3ms/step - loss: 0.0283 - RMSE: 0.1349 - val_loss: 0.0296 - val_RMSE: 0.1368
Epoch 9/20
25/25 [==============================] - 0s 4ms/step - loss: 0.0268 - RMSE: 0.1314 - val_loss: 0.0287 - val_RMSE: 0.1361
Epoch 10/20
25/25 [==============================] - 0s 5ms/step - loss: 0.0260 - RMSE: 0.1305 - val_loss: 0.0287 - val_RMSE: 0.1385
Epoch 11/20
25/25 [==============================] - 0s 8ms/step - loss: 0.0260 - RMSE: 0.1324 - val_loss: 0.0278 - val_RMSE: 0.1369
Epoch 12/20
25/25 [==============================] - 0s 7ms/step - loss: 0.0254 - RMSE: 0.1321 - val_loss: 0.0279 - val_RMSE: 0.1386
Epoch 13/20
25/25 [==============================] - 0s 4ms/step - loss: 0.0250 - RMSE: 0.1321 - val_loss: 0.0301 - val_RMSE: 0.1478
Epoch 14/20
25/25 [==============================] - 0s 9ms/step - loss: 0.0250 - RMSE: 0.1337 - val_loss: 0.0321 - val_RMSE: 0.1589
Epoch 15/20
25/25 [==============================] - 0s 9ms/step - loss: 0.0255 - RMSE: 0.1366 - val_loss: 0.0291 - val_RMSE: 0.1466
Epoch 16/20
25/25 [==============================] - 0s 4ms/step - loss: 0.0246 - RMSE: 0.1348 - val_loss: 0.0256 - val_RMSE: 0.1382
Epoch 17/20
25/25 [==============================] - 0s 3ms/step - loss: 0.0234 - RMSE: 0.1319 - val_loss: 0.0252 - val_RMSE: 0.1388
Epoch 18/20
25/25 [==============================] - 0s 3ms/step - loss: 0.0230 - RMSE: 0.1317 - val_loss: 0.0264 - val_RMSE: 0.1412
Epoch 19/20
25/25 [==============================] - 0s 3ms/step - loss: 0.0228 - RMSE: 0.1322 - val_loss: 0.0262 - val_RMSE: 0.1411
Epoch 20/20
25/25 [==============================] - 0s 3ms/step - loss: 0.0231 - RMSE: 0.1348 - val_loss: 0.0252 - val_RMSE: 0.1410

============================ Beginning trial with hyperparams: OPTIMIZER: Adam, LEARNING RATE: 0.05, EPOCHS: 20============================

Epoch 1/20
13/13 [==============================] - 1s 20ms/step - loss: 0.5542 - RMSE: 0.6007 - val_loss: 0.1299 - val_RMSE: 0.3357
Epoch 2/20
13/13 [==============================] - 0s 10ms/step - loss: 0.0592 - RMSE: 0.2133 - val_loss: 0.0601 - val_RMSE: 0.2090
Epoch 3/20
13/13 [==============================] - 0s 14ms/step - loss: 0.0379 - RMSE: 0.1569 - val_loss: 0.0417 - val_RMSE: 0.1579
Epoch 4/20
13/13 [==============================] - 0s 6ms/step - loss: 0.0330 - RMSE: 0.1398 - val_loss: 0.0347 - val_RMSE: 0.1375
Epoch 5/20
13/13 [==============================] - 0s 5ms/step - loss: 0.0326 - RMSE: 0.1385 - val_loss: 0.0339 - val_RMSE: 0.1351
Epoch 6/20
13/13 [==============================] - 0s 5ms/step - loss: 0.0310 - RMSE: 0.1344 - val_loss: 0.0342 - val_RMSE: 0.1363
Epoch 7/20
13/13 [==============================] - 0s 5ms/step - loss: 0.0313 - RMSE: 0.1350 - val_loss: 0.0327 - val_RMSE: 0.1353
Epoch 8/20
13/13 [==============================] - 0s 5ms/step - loss: 0.0300 - RMSE: 0.1329 - val_loss: 0.0328 - val_RMSE: 0.1366
Epoch 9/20
13/13 [==============================] - 0s 8ms/step - loss: 0.0293 - RMSE: 0.1312 - val_loss: 0.0326 - val_RMSE: 0.1353
Epoch 10/20
13/13 [==============================] - 0s 9ms/step - loss: 0.0299 - RMSE: 0.1338 - val_loss: 0.0320 - val_RMSE: 0.1366
Epoch 11/20
13/13 [==============================] - 0s 11ms/step - loss: 0.0284 - RMSE: 0.1302 - val_loss: 0.0329 - val_RMSE: 0.1421
Epoch 12/20
13/13 [==============================] - 0s 10ms/step - loss: 0.0298 - RMSE: 0.1358 - val_loss: 0.0312 - val_RMSE: 0.1356
Epoch 13/20
13/13 [==============================] - 0s 7ms/step - loss: 0.0284 - RMSE: 0.1332 - val_loss: 0.0309 - val_RMSE: 0.1326
Epoch 14/20
13/13 [==============================] - 0s 5ms/step - loss: 0.0276 - RMSE: 0.1315 - val_loss: 0.0309 - val_RMSE: 0.1393
Epoch 15/20
13/13 [==============================] - 0s 4ms/step - loss: 0.0284 - RMSE: 0.1350 - val_loss: 0.0298 - val_RMSE: 0.1336
Epoch 16/20
13/13 [==============================] - 0s 4ms/step - loss: 0.0287 - RMSE: 0.1357 - val_loss: 0.0310 - val_RMSE: 0.1411
Epoch 17/20
13/13 [==============================] - 0s 4ms/step - loss: 0.0270 - RMSE: 0.1325 - val_loss: 0.0289 - val_RMSE: 0.1349
Epoch 18/20
13/13 [==============================] - 0s 5ms/step - loss: 0.0256 - RMSE: 0.1278 - val_loss: 0.0288 - val_RMSE: 0.1343
Epoch 19/20
13/13 [==============================] - 0s 5ms/step - loss: 0.0255 - RMSE: 0.1292 - val_loss: 0.0286 - val_RMSE: 0.1351
Epoch 20/20
13/13 [==============================] - 0s 5ms/step - loss: 0.0249 - RMSE: 0.1278 - val_loss: 0.0279 - val_RMSE: 0.1348

============================ Beginning trial with hyperparams: OPTIMIZER: Adam, LEARNING RATE: 0.05, EPOCHS: 40============================

Epoch 1/40
100/100 [==============================] - 1s 5ms/step - loss: 0.1205 - RMSE: 0.2401 - val_loss: 0.0345 - val_RMSE: 0.1436
Epoch 2/40
100/100 [==============================] - 0s 5ms/step - loss: 0.0324 - RMSE: 0.1401 - val_loss: 0.0336 - val_RMSE: 0.1449
Epoch 3/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0312 - RMSE: 0.1406 - val_loss: 0.0338 - val_RMSE: 0.1500
Epoch 4/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0293 - RMSE: 0.1387 - val_loss: 0.0293 - val_RMSE: 0.1403
Epoch 5/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0275 - RMSE: 0.1371 - val_loss: 0.0282 - val_RMSE: 0.1405
Epoch 6/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0269 - RMSE: 0.1395 - val_loss: 0.0268 - val_RMSE: 0.1401
Epoch 7/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0279 - RMSE: 0.1466 - val_loss: 0.0282 - val_RMSE: 0.1487
Epoch 8/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0270 - RMSE: 0.1483 - val_loss: 0.0282 - val_RMSE: 0.1530
Epoch 9/40
100/100 [==============================] - 0s 2ms/step - loss: 0.0257 - RMSE: 0.1468 - val_loss: 0.0264 - val_RMSE: 0.1487
Epoch 10/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0252 - RMSE: 0.1468 - val_loss: 0.0278 - val_RMSE: 0.1539
Epoch 11/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0252 - RMSE: 0.1484 - val_loss: 0.0260 - val_RMSE: 0.1506
Epoch 12/40
100/100 [==============================] - 0s 5ms/step - loss: 0.0238 - RMSE: 0.1461 - val_loss: 0.0251 - val_RMSE: 0.1490
Epoch 13/40
100/100 [==============================] - 0s 5ms/step - loss: 0.0236 - RMSE: 0.1462 - val_loss: 0.0250 - val_RMSE: 0.1495
Epoch 14/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0237 - RMSE: 0.1474 - val_loss: 0.0246 - val_RMSE: 0.1489
Epoch 15/40
100/100 [==============================] - 0s 3ms/step - loss: 0.0240 - RMSE: 0.1484 - val_loss: 0.0253 - val_RMSE: 0.1513
Epoch 16/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0233 - RMSE: 0.1473 - val_loss: 0.0253 - val_RMSE: 0.1516
Epoch 17/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0230 - RMSE: 0.1454 - val_loss: 0.0263 - val_RMSE: 0.1549
Epoch 18/40
100/100 [==============================] - 0s 5ms/step - loss: 0.0237 - RMSE: 0.1486 - val_loss: 0.0249 - val_RMSE: 0.1510
Epoch 19/40
100/100 [==============================] - 0s 5ms/step - loss: 0.0247 - RMSE: 0.1524 - val_loss: 0.0240 - val_RMSE: 0.1493
Epoch 20/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0235 - RMSE: 0.1480 - val_loss: 0.0264 - val_RMSE: 0.1554
Epoch 21/40
100/100 [==============================] - 0s 5ms/step - loss: 0.0235 - RMSE: 0.1486 - val_loss: 0.0245 - val_RMSE: 0.1501
Epoch 22/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0234 - RMSE: 0.1485 - val_loss: 0.0239 - val_RMSE: 0.1491
Epoch 23/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0232 - RMSE: 0.1475 - val_loss: 0.0264 - val_RMSE: 0.1558
Epoch 24/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0229 - RMSE: 0.1472 - val_loss: 0.0239 - val_RMSE: 0.1486
Epoch 25/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0231 - RMSE: 0.1480 - val_loss: 0.0242 - val_RMSE: 0.1493
Epoch 26/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0230 - RMSE: 0.1466 - val_loss: 0.0240 - val_RMSE: 0.1495
Epoch 27/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0232 - RMSE: 0.1485 - val_loss: 0.0248 - val_RMSE: 0.1512
Epoch 28/40
100/100 [==============================] - 0s 5ms/step - loss: 0.0232 - RMSE: 0.1478 - val_loss: 0.0262 - val_RMSE: 0.1567
Epoch 29/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0236 - RMSE: 0.1497 - val_loss: 0.0242 - val_RMSE: 0.1496
Epoch 30/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0229 - RMSE: 0.1475 - val_loss: 0.0278 - val_RMSE: 0.1597
Epoch 31/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0230 - RMSE: 0.1472 - val_loss: 0.0257 - val_RMSE: 0.1540
Epoch 32/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0230 - RMSE: 0.1478 - val_loss: 0.0238 - val_RMSE: 0.1483
Epoch 33/40
100/100 [==============================] - 0s 5ms/step - loss: 0.0229 - RMSE: 0.1469 - val_loss: 0.0257 - val_RMSE: 0.1537
Epoch 34/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0234 - RMSE: 0.1480 - val_loss: 0.0250 - val_RMSE: 0.1514
Epoch 35/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0228 - RMSE: 0.1461 - val_loss: 0.0263 - val_RMSE: 0.1554
Epoch 36/40
100/100 [==============================] - 0s 3ms/step - loss: 0.0233 - RMSE: 0.1485 - val_loss: 0.0240 - val_RMSE: 0.1493
Epoch 37/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0237 - RMSE: 0.1488 - val_loss: 0.0239 - val_RMSE: 0.1485
Epoch 38/40
100/100 [==============================] - 0s 5ms/step - loss: 0.0228 - RMSE: 0.1462 - val_loss: 0.0288 - val_RMSE: 0.1628
Epoch 39/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0231 - RMSE: 0.1478 - val_loss: 0.0248 - val_RMSE: 0.1512
Epoch 40/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0230 - RMSE: 0.1468 - val_loss: 0.0247 - val_RMSE: 0.1516

============================ Beginning trial with hyperparams: OPTIMIZER: Adam, LEARNING RATE: 0.05, EPOCHS: 40============================

Epoch 1/40
25/25 [==============================] - 1s 11ms/step - loss: 0.2999 - RMSE: 0.4186 - val_loss: 0.0564 - val_RMSE: 0.2015
Epoch 2/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0361 - RMSE: 0.1520 - val_loss: 0.0356 - val_RMSE: 0.1474
Epoch 3/40
25/25 [==============================] - 0s 6ms/step - loss: 0.0318 - RMSE: 0.1383 - val_loss: 0.0348 - val_RMSE: 0.1457
Epoch 4/40
25/25 [==============================] - 0s 4ms/step - loss: 0.0307 - RMSE: 0.1358 - val_loss: 0.0364 - val_RMSE: 0.1538
Epoch 5/40
25/25 [==============================] - 0s 9ms/step - loss: 0.0305 - RMSE: 0.1366 - val_loss: 0.0327 - val_RMSE: 0.1431
Epoch 6/40
25/25 [==============================] - 0s 8ms/step - loss: 0.0289 - RMSE: 0.1328 - val_loss: 0.0315 - val_RMSE: 0.1402
Epoch 7/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0295 - RMSE: 0.1370 - val_loss: 0.0305 - val_RMSE: 0.1392
Epoch 8/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0287 - RMSE: 0.1359 - val_loss: 0.0302 - val_RMSE: 0.1390
Epoch 9/40
25/25 [==============================] - 0s 6ms/step - loss: 0.0266 - RMSE: 0.1304 - val_loss: 0.0307 - val_RMSE: 0.1449
Epoch 10/40
25/25 [==============================] - 0s 8ms/step - loss: 0.0266 - RMSE: 0.1325 - val_loss: 0.0280 - val_RMSE: 0.1361
Epoch 11/40
25/25 [==============================] - 0s 8ms/step - loss: 0.0252 - RMSE: 0.1295 - val_loss: 0.0282 - val_RMSE: 0.1391
Epoch 12/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0259 - RMSE: 0.1339 - val_loss: 0.0267 - val_RMSE: 0.1355
Epoch 13/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0245 - RMSE: 0.1306 - val_loss: 0.0280 - val_RMSE: 0.1422
Epoch 14/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0243 - RMSE: 0.1316 - val_loss: 0.0263 - val_RMSE: 0.1385
Epoch 15/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0243 - RMSE: 0.1333 - val_loss: 0.0257 - val_RMSE: 0.1362
Epoch 16/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0225 - RMSE: 0.1286 - val_loss: 0.0248 - val_RMSE: 0.1359
Epoch 17/40
25/25 [==============================] - 0s 6ms/step - loss: 0.0221 - RMSE: 0.1287 - val_loss: 0.0252 - val_RMSE: 0.1405
Epoch 18/40
25/25 [==============================] - 0s 8ms/step - loss: 0.0220 - RMSE: 0.1297 - val_loss: 0.0240 - val_RMSE: 0.1367
Epoch 19/40
25/25 [==============================] - 0s 8ms/step - loss: 0.0220 - RMSE: 0.1311 - val_loss: 0.0242 - val_RMSE: 0.1371
Epoch 20/40
25/25 [==============================] - 0s 5ms/step - loss: 0.0217 - RMSE: 0.1315 - val_loss: 0.0240 - val_RMSE: 0.1389
Epoch 21/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0221 - RMSE: 0.1338 - val_loss: 0.0245 - val_RMSE: 0.1421
Epoch 22/40
25/25 [==============================] - 0s 5ms/step - loss: 0.0221 - RMSE: 0.1352 - val_loss: 0.0234 - val_RMSE: 0.1384
Epoch 23/40
25/25 [==============================] - 0s 8ms/step - loss: 0.0211 - RMSE: 0.1322 - val_loss: 0.0224 - val_RMSE: 0.1365
Epoch 24/40
25/25 [==============================] - 0s 5ms/step - loss: 0.0203 - RMSE: 0.1301 - val_loss: 0.0222 - val_RMSE: 0.1373
Epoch 25/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0202 - RMSE: 0.1307 - val_loss: 0.0222 - val_RMSE: 0.1383
Epoch 26/40
25/25 [==============================] - 0s 4ms/step - loss: 0.0199 - RMSE: 0.1308 - val_loss: 0.0219 - val_RMSE: 0.1376
Epoch 27/40
25/25 [==============================] - 0s 4ms/step - loss: 0.0199 - RMSE: 0.1315 - val_loss: 0.0218 - val_RMSE: 0.1358
Epoch 28/40
25/25 [==============================] - 0s 5ms/step - loss: 0.0205 - RMSE: 0.1339 - val_loss: 0.0226 - val_RMSE: 0.1415
Epoch 29/40
25/25 [==============================] - 0s 4ms/step - loss: 0.0198 - RMSE: 0.1322 - val_loss: 0.0221 - val_RMSE: 0.1375
Epoch 30/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0194 - RMSE: 0.1309 - val_loss: 0.0214 - val_RMSE: 0.1389
Epoch 31/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0196 - RMSE: 0.1320 - val_loss: 0.0212 - val_RMSE: 0.1394
Epoch 32/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0192 - RMSE: 0.1315 - val_loss: 0.0222 - val_RMSE: 0.1425
Epoch 33/40
25/25 [==============================] - 0s 4ms/step - loss: 0.0199 - RMSE: 0.1344 - val_loss: 0.0207 - val_RMSE: 0.1383
Epoch 34/40
25/25 [==============================] - 0s 4ms/step - loss: 0.0189 - RMSE: 0.1308 - val_loss: 0.0208 - val_RMSE: 0.1373
Epoch 35/40
25/25 [==============================] - 0s 5ms/step - loss: 0.0182 - RMSE: 0.1286 - val_loss: 0.0248 - val_RMSE: 0.1525
Epoch 36/40
25/25 [==============================] - 0s 5ms/step - loss: 0.0195 - RMSE: 0.1334 - val_loss: 0.0210 - val_RMSE: 0.1388
Epoch 37/40
25/25 [==============================] - 0s 8ms/step - loss: 0.0190 - RMSE: 0.1328 - val_loss: 0.0203 - val_RMSE: 0.1378
Epoch 38/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0184 - RMSE: 0.1301 - val_loss: 0.0197 - val_RMSE: 0.1367
Epoch 39/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0183 - RMSE: 0.1301 - val_loss: 0.0210 - val_RMSE: 0.1386
Epoch 40/40
25/25 [==============================] - 0s 4ms/step - loss: 0.0188 - RMSE: 0.1323 - val_loss: 0.0223 - val_RMSE: 0.1472

============================ Beginning trial with hyperparams: OPTIMIZER: Adam, LEARNING RATE: 0.05, EPOCHS: 40============================

Epoch 1/40
13/13 [==============================] - 1s 13ms/step - loss: 0.4999 - RMSE: 0.5649 - val_loss: 0.1666 - val_RMSE: 0.3831
Epoch 2/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0565 - RMSE: 0.2073 - val_loss: 0.0518 - val_RMSE: 0.1912
Epoch 3/40
13/13 [==============================] - 0s 5ms/step - loss: 0.0439 - RMSE: 0.1740 - val_loss: 0.0498 - val_RMSE: 0.1859
Epoch 4/40
13/13 [==============================] - 0s 7ms/step - loss: 0.0376 - RMSE: 0.1550 - val_loss: 0.0488 - val_RMSE: 0.1812
Epoch 5/40
13/13 [==============================] - 0s 7ms/step - loss: 0.0347 - RMSE: 0.1454 - val_loss: 0.0354 - val_RMSE: 0.1396
Epoch 6/40
13/13 [==============================] - 0s 6ms/step - loss: 0.0319 - RMSE: 0.1368 - val_loss: 0.0350 - val_RMSE: 0.1414
Epoch 7/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0318 - RMSE: 0.1374 - val_loss: 0.0382 - val_RMSE: 0.1530
Epoch 8/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0307 - RMSE: 0.1356 - val_loss: 0.0349 - val_RMSE: 0.1418
Epoch 9/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0298 - RMSE: 0.1325 - val_loss: 0.0330 - val_RMSE: 0.1368
Epoch 10/40
13/13 [==============================] - 0s 5ms/step - loss: 0.0294 - RMSE: 0.1313 - val_loss: 0.0324 - val_RMSE: 0.1371
Epoch 11/40
13/13 [==============================] - 0s 7ms/step - loss: 0.0292 - RMSE: 0.1332 - val_loss: 0.0326 - val_RMSE: 0.1379
Epoch 12/40
13/13 [==============================] - 0s 8ms/step - loss: 0.0283 - RMSE: 0.1298 - val_loss: 0.0321 - val_RMSE: 0.1400
Epoch 13/40
13/13 [==============================] - 0s 8ms/step - loss: 0.0283 - RMSE: 0.1317 - val_loss: 0.0309 - val_RMSE: 0.1346
Epoch 14/40
13/13 [==============================] - 0s 7ms/step - loss: 0.0286 - RMSE: 0.1333 - val_loss: 0.0304 - val_RMSE: 0.1348
Epoch 15/40
13/13 [==============================] - 0s 5ms/step - loss: 0.0278 - RMSE: 0.1326 - val_loss: 0.0316 - val_RMSE: 0.1406
Epoch 16/40
13/13 [==============================] - 0s 6ms/step - loss: 0.0277 - RMSE: 0.1333 - val_loss: 0.0296 - val_RMSE: 0.1350
Epoch 17/40
13/13 [==============================] - 0s 6ms/step - loss: 0.0264 - RMSE: 0.1295 - val_loss: 0.0296 - val_RMSE: 0.1360
Epoch 18/40
13/13 [==============================] - 0s 9ms/step - loss: 0.0267 - RMSE: 0.1316 - val_loss: 0.0296 - val_RMSE: 0.1370
Epoch 19/40
13/13 [==============================] - 0s 7ms/step - loss: 0.0261 - RMSE: 0.1308 - val_loss: 0.0289 - val_RMSE: 0.1366
Epoch 20/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0256 - RMSE: 0.1295 - val_loss: 0.0278 - val_RMSE: 0.1332
Epoch 21/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0250 - RMSE: 0.1288 - val_loss: 0.0277 - val_RMSE: 0.1363
Epoch 22/40
13/13 [==============================] - 0s 7ms/step - loss: 0.0246 - RMSE: 0.1284 - val_loss: 0.0279 - val_RMSE: 0.1376
Epoch 23/40
13/13 [==============================] - 0s 14ms/step - loss: 0.0249 - RMSE: 0.1312 - val_loss: 0.0276 - val_RMSE: 0.1348
Epoch 24/40
13/13 [==============================] - 0s 12ms/step - loss: 0.0242 - RMSE: 0.1293 - val_loss: 0.0271 - val_RMSE: 0.1370
Epoch 25/40
13/13 [==============================] - 0s 10ms/step - loss: 0.0239 - RMSE: 0.1288 - val_loss: 0.0271 - val_RMSE: 0.1368
Epoch 26/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0238 - RMSE: 0.1296 - val_loss: 0.0267 - val_RMSE: 0.1356
Epoch 27/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0236 - RMSE: 0.1302 - val_loss: 0.0261 - val_RMSE: 0.1350
Epoch 28/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0236 - RMSE: 0.1303 - val_loss: 0.0256 - val_RMSE: 0.1341
Epoch 29/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0225 - RMSE: 0.1268 - val_loss: 0.0253 - val_RMSE: 0.1352
Epoch 30/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0225 - RMSE: 0.1291 - val_loss: 0.0260 - val_RMSE: 0.1362
Epoch 31/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0237 - RMSE: 0.1342 - val_loss: 0.0248 - val_RMSE: 0.1362
Epoch 32/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0229 - RMSE: 0.1310 - val_loss: 0.0251 - val_RMSE: 0.1354
Epoch 33/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0218 - RMSE: 0.1282 - val_loss: 0.0245 - val_RMSE: 0.1357
Epoch 34/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0214 - RMSE: 0.1283 - val_loss: 0.0256 - val_RMSE: 0.1415
Epoch 35/40
13/13 [==============================] - 0s 6ms/step - loss: 0.0213 - RMSE: 0.1286 - val_loss: 0.0251 - val_RMSE: 0.1400
Epoch 36/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0213 - RMSE: 0.1290 - val_loss: 0.0248 - val_RMSE: 0.1399
Epoch 37/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0213 - RMSE: 0.1300 - val_loss: 0.0239 - val_RMSE: 0.1357
Epoch 38/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0215 - RMSE: 0.1322 - val_loss: 0.0251 - val_RMSE: 0.1391
Epoch 39/40
13/13 [==============================] - 0s 5ms/step - loss: 0.0215 - RMSE: 0.1317 - val_loss: 0.0237 - val_RMSE: 0.1367
Epoch 40/40
13/13 [==============================] - 0s 6ms/step - loss: 0.0207 - RMSE: 0.1301 - val_loss: 0.0243 - val_RMSE: 0.1391

============================ Beginning trial with hyperparams: OPTIMIZER: Adam, LEARNING RATE: 0.001, EPOCHS: 20============================

Epoch 1/20
100/100 [==============================] - 1s 6ms/step - loss: 0.2612 - RMSE: 0.4725 - val_loss: 0.3172 - val_RMSE: 0.5509
Epoch 2/20
100/100 [==============================] - 1s 5ms/step - loss: 0.1212 - RMSE: 0.3263 - val_loss: 0.2070 - val_RMSE: 0.4414
Epoch 3/20
100/100 [==============================] - 0s 3ms/step - loss: 0.0949 - RMSE: 0.2858 - val_loss: 0.1315 - val_RMSE: 0.3469
Epoch 4/20
100/100 [==============================] - 1s 5ms/step - loss: 0.0778 - RMSE: 0.2578 - val_loss: 0.0674 - val_RMSE: 0.2392
Epoch 5/20
100/100 [==============================] - 0s 4ms/step - loss: 0.0664 - RMSE: 0.2357 - val_loss: 0.0453 - val_RMSE: 0.1901
Epoch 6/20
100/100 [==============================] - 0s 4ms/step - loss: 0.0606 - RMSE: 0.2243 - val_loss: 0.0389 - val_RMSE: 0.1726
Epoch 7/20
100/100 [==============================] - 0s 4ms/step - loss: 0.0517 - RMSE: 0.2041 - val_loss: 0.0387 - val_RMSE: 0.1725
Epoch 8/20
100/100 [==============================] - 0s 4ms/step - loss: 0.0529 - RMSE: 0.2081 - val_loss: 0.0385 - val_RMSE: 0.1715
Epoch 9/20
100/100 [==============================] - 0s 4ms/step - loss: 0.0454 - RMSE: 0.1909 - val_loss: 0.0406 - val_RMSE: 0.1769
Epoch 10/20
100/100 [==============================] - 0s 5ms/step - loss: 0.0440 - RMSE: 0.1874 - val_loss: 0.0341 - val_RMSE: 0.1591
Epoch 11/20
100/100 [==============================] - 0s 4ms/step - loss: 0.0402 - RMSE: 0.1768 - val_loss: 0.0360 - val_RMSE: 0.1643
Epoch 12/20
100/100 [==============================] - 0s 5ms/step - loss: 0.0393 - RMSE: 0.1750 - val_loss: 0.0378 - val_RMSE: 0.1689
Epoch 13/20
100/100 [==============================] - 0s 3ms/step - loss: 0.0379 - RMSE: 0.1709 - val_loss: 0.0338 - val_RMSE: 0.1584
Epoch 14/20
100/100 [==============================] - 0s 4ms/step - loss: 0.0367 - RMSE: 0.1688 - val_loss: 0.0328 - val_RMSE: 0.1546
Epoch 15/20
100/100 [==============================] - 0s 4ms/step - loss: 0.0337 - RMSE: 0.1608 - val_loss: 0.0312 - val_RMSE: 0.1495
Epoch 16/20
100/100 [==============================] - 0s 4ms/step - loss: 0.0335 - RMSE: 0.1591 - val_loss: 0.0323 - val_RMSE: 0.1549
Epoch 17/20
100/100 [==============================] - 0s 4ms/step - loss: 0.0327 - RMSE: 0.1565 - val_loss: 0.0324 - val_RMSE: 0.1543
Epoch 18/20
100/100 [==============================] - 0s 3ms/step - loss: 0.0312 - RMSE: 0.1523 - val_loss: 0.0308 - val_RMSE: 0.1497
Epoch 19/20
100/100 [==============================] - 0s 4ms/step - loss: 0.0313 - RMSE: 0.1534 - val_loss: 0.0306 - val_RMSE: 0.1492
Epoch 20/20
100/100 [==============================] - 0s 3ms/step - loss: 0.0304 - RMSE: 0.1515 - val_loss: 0.0324 - val_RMSE: 0.1553

============================ Beginning trial with hyperparams: OPTIMIZER: Adam, LEARNING RATE: 0.001, EPOCHS: 20============================

Epoch 1/20
25/25 [==============================] - 1s 12ms/step - loss: 0.5176 - RMSE: 0.6675 - val_loss: 0.3315 - val_RMSE: 0.5694
Epoch 2/20
25/25 [==============================] - 0s 8ms/step - loss: 0.1760 - RMSE: 0.4059 - val_loss: 0.2940 - val_RMSE: 0.5357
Epoch 3/20
25/25 [==============================] - 0s 6ms/step - loss: 0.1348 - RMSE: 0.3518 - val_loss: 0.2727 - val_RMSE: 0.5159
Epoch 4/20
25/25 [==============================] - 0s 3ms/step - loss: 0.1161 - RMSE: 0.3264 - val_loss: 0.2490 - val_RMSE: 0.4909
Epoch 5/20
25/25 [==============================] - 0s 3ms/step - loss: 0.1041 - RMSE: 0.3076 - val_loss: 0.2233 - val_RMSE: 0.4632
Epoch 6/20
25/25 [==============================] - 0s 7ms/step - loss: 0.0920 - RMSE: 0.2879 - val_loss: 0.2004 - val_RMSE: 0.4380
Epoch 7/20
25/25 [==============================] - 0s 4ms/step - loss: 0.0860 - RMSE: 0.2770 - val_loss: 0.1850 - val_RMSE: 0.4214
Epoch 8/20
25/25 [==============================] - 0s 3ms/step - loss: 0.0809 - RMSE: 0.2682 - val_loss: 0.1611 - val_RMSE: 0.3901
Epoch 9/20
25/25 [==============================] - 0s 3ms/step - loss: 0.0744 - RMSE: 0.2565 - val_loss: 0.1460 - val_RMSE: 0.3709
Epoch 10/20
25/25 [==============================] - 0s 3ms/step - loss: 0.0690 - RMSE: 0.2460 - val_loss: 0.1251 - val_RMSE: 0.3415
Epoch 11/20
25/25 [==============================] - 0s 3ms/step - loss: 0.0656 - RMSE: 0.2387 - val_loss: 0.1101 - val_RMSE: 0.3187
Epoch 12/20
25/25 [==============================] - 0s 4ms/step - loss: 0.0626 - RMSE: 0.2328 - val_loss: 0.0999 - val_RMSE: 0.3021
Epoch 13/20
25/25 [==============================] - 0s 7ms/step - loss: 0.0659 - RMSE: 0.2385 - val_loss: 0.0811 - val_RMSE: 0.2705
Epoch 14/20
25/25 [==============================] - 0s 5ms/step - loss: 0.0561 - RMSE: 0.2194 - val_loss: 0.0708 - val_RMSE: 0.2500
Epoch 15/20
25/25 [==============================] - 0s 4ms/step - loss: 0.0565 - RMSE: 0.2200 - val_loss: 0.0630 - val_RMSE: 0.2353
Epoch 16/20
25/25 [==============================] - 0s 8ms/step - loss: 0.0538 - RMSE: 0.2142 - val_loss: 0.0511 - val_RMSE: 0.2079
Epoch 17/20
25/25 [==============================] - 0s 4ms/step - loss: 0.0530 - RMSE: 0.2115 - val_loss: 0.0465 - val_RMSE: 0.1976
Epoch 18/20
25/25 [==============================] - 0s 4ms/step - loss: 0.0524 - RMSE: 0.2113 - val_loss: 0.0414 - val_RMSE: 0.1862
Epoch 19/20
25/25 [==============================] - 0s 3ms/step - loss: 0.0472 - RMSE: 0.1994 - val_loss: 0.0371 - val_RMSE: 0.1725
Epoch 20/20
25/25 [==============================] - 0s 3ms/step - loss: 0.0466 - RMSE: 0.1973 - val_loss: 0.0363 - val_RMSE: 0.1698

============================ Beginning trial with hyperparams: OPTIMIZER: Adam, LEARNING RATE: 0.001, EPOCHS: 20============================

Epoch 1/20
13/13 [==============================] - 1s 21ms/step - loss: 0.7102 - RMSE: 0.8087 - val_loss: 0.3471 - val_RMSE: 0.5811
Epoch 2/20
13/13 [==============================] - 0s 7ms/step - loss: 0.2870 - RMSE: 0.5195 - val_loss: 0.2891 - val_RMSE: 0.5249
Epoch 3/20
13/13 [==============================] - 0s 8ms/step - loss: 0.1834 - RMSE: 0.4172 - val_loss: 0.2748 - val_RMSE: 0.5095
Epoch 4/20
13/13 [==============================] - 0s 7ms/step - loss: 0.1407 - RMSE: 0.3633 - val_loss: 0.2590 - val_RMSE: 0.4933
Epoch 5/20
13/13 [==============================] - 0s 4ms/step - loss: 0.1182 - RMSE: 0.3310 - val_loss: 0.2479 - val_RMSE: 0.4827
Epoch 6/20
13/13 [==============================] - 0s 4ms/step - loss: 0.1031 - RMSE: 0.3085 - val_loss: 0.2445 - val_RMSE: 0.4792
Epoch 7/20
13/13 [==============================] - 0s 4ms/step - loss: 0.0990 - RMSE: 0.3030 - val_loss: 0.2350 - val_RMSE: 0.4685
Epoch 8/20
13/13 [==============================] - 0s 4ms/step - loss: 0.0887 - RMSE: 0.2840 - val_loss: 0.2277 - val_RMSE: 0.4606
Epoch 9/20
13/13 [==============================] - 0s 4ms/step - loss: 0.0877 - RMSE: 0.2823 - val_loss: 0.2199 - val_RMSE: 0.4518
Epoch 10/20
13/13 [==============================] - 0s 4ms/step - loss: 0.0777 - RMSE: 0.2648 - val_loss: 0.2035 - val_RMSE: 0.4333
Epoch 11/20
13/13 [==============================] - 0s 4ms/step - loss: 0.0751 - RMSE: 0.2591 - val_loss: 0.2024 - val_RMSE: 0.4310
Epoch 12/20
13/13 [==============================] - 0s 4ms/step - loss: 0.0722 - RMSE: 0.2546 - val_loss: 0.1928 - val_RMSE: 0.4202
Epoch 13/20
13/13 [==============================] - 0s 4ms/step - loss: 0.0719 - RMSE: 0.2544 - val_loss: 0.1865 - val_RMSE: 0.4140
Epoch 14/20
13/13 [==============================] - 0s 4ms/step - loss: 0.0674 - RMSE: 0.2459 - val_loss: 0.1746 - val_RMSE: 0.3983
Epoch 15/20
13/13 [==============================] - 0s 5ms/step - loss: 0.0636 - RMSE: 0.2374 - val_loss: 0.1662 - val_RMSE: 0.3877
Epoch 16/20
13/13 [==============================] - 0s 5ms/step - loss: 0.0603 - RMSE: 0.2312 - val_loss: 0.1572 - val_RMSE: 0.3764
Epoch 17/20
13/13 [==============================] - 0s 4ms/step - loss: 0.0576 - RMSE: 0.2266 - val_loss: 0.1478 - val_RMSE: 0.3636
Epoch 18/20
13/13 [==============================] - 0s 5ms/step - loss: 0.0573 - RMSE: 0.2249 - val_loss: 0.1438 - val_RMSE: 0.3579
Epoch 19/20
13/13 [==============================] - 0s 5ms/step - loss: 0.0571 - RMSE: 0.2251 - val_loss: 0.1292 - val_RMSE: 0.3379
Epoch 20/20
13/13 [==============================] - 0s 5ms/step - loss: 0.0560 - RMSE: 0.2225 - val_loss: 0.1203 - val_RMSE: 0.3244

============================ Beginning trial with hyperparams: OPTIMIZER: Adam, LEARNING RATE: 0.001, EPOCHS: 40============================

Epoch 1/40
100/100 [==============================] - 1s 5ms/step - loss: 0.4213 - RMSE: 0.5804 - val_loss: 0.1801 - val_RMSE: 0.4097
Epoch 2/40
100/100 [==============================] - 0s 4ms/step - loss: 0.1344 - RMSE: 0.3468 - val_loss: 0.1299 - val_RMSE: 0.3468
Epoch 3/40
100/100 [==============================] - 0s 3ms/step - loss: 0.1017 - RMSE: 0.2978 - val_loss: 0.0818 - val_RMSE: 0.2704
Epoch 4/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0806 - RMSE: 0.2640 - val_loss: 0.0512 - val_RMSE: 0.2066
Epoch 5/40
100/100 [==============================] - 0s 3ms/step - loss: 0.0710 - RMSE: 0.2454 - val_loss: 0.0377 - val_RMSE: 0.1714
Epoch 6/40
100/100 [==============================] - 0s 5ms/step - loss: 0.0629 - RMSE: 0.2309 - val_loss: 0.0382 - val_RMSE: 0.1712
Epoch 7/40
100/100 [==============================] - 0s 5ms/step - loss: 0.0530 - RMSE: 0.2093 - val_loss: 0.0398 - val_RMSE: 0.1766
Epoch 8/40
100/100 [==============================] - 1s 5ms/step - loss: 0.0529 - RMSE: 0.2092 - val_loss: 0.0437 - val_RMSE: 0.1860
Epoch 9/40
100/100 [==============================] - 0s 3ms/step - loss: 0.0479 - RMSE: 0.1976 - val_loss: 0.0448 - val_RMSE: 0.1883
Epoch 10/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0466 - RMSE: 0.1952 - val_loss: 0.0423 - val_RMSE: 0.1833
Epoch 11/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0415 - RMSE: 0.1813 - val_loss: 0.0390 - val_RMSE: 0.1743
Epoch 12/40
100/100 [==============================] - 0s 5ms/step - loss: 0.0406 - RMSE: 0.1801 - val_loss: 0.0367 - val_RMSE: 0.1671
Epoch 13/40
100/100 [==============================] - 0s 2ms/step - loss: 0.0399 - RMSE: 0.1781 - val_loss: 0.0336 - val_RMSE: 0.1592
Epoch 14/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0365 - RMSE: 0.1698 - val_loss: 0.0346 - val_RMSE: 0.1620
Epoch 15/40
100/100 [==============================] - 0s 5ms/step - loss: 0.0368 - RMSE: 0.1699 - val_loss: 0.0365 - val_RMSE: 0.1683
Epoch 16/40
100/100 [==============================] - 0s 3ms/step - loss: 0.0354 - RMSE: 0.1663 - val_loss: 0.0369 - val_RMSE: 0.1702
Epoch 17/40
100/100 [==============================] - 0s 5ms/step - loss: 0.0324 - RMSE: 0.1585 - val_loss: 0.0313 - val_RMSE: 0.1539
Epoch 18/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0333 - RMSE: 0.1609 - val_loss: 0.0311 - val_RMSE: 0.1526
Epoch 19/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0329 - RMSE: 0.1598 - val_loss: 0.0300 - val_RMSE: 0.1490
Epoch 20/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0308 - RMSE: 0.1536 - val_loss: 0.0287 - val_RMSE: 0.1446
Epoch 21/40
100/100 [==============================] - 0s 2ms/step - loss: 0.0305 - RMSE: 0.1532 - val_loss: 0.0278 - val_RMSE: 0.1412
Epoch 22/40
100/100 [==============================] - 0s 3ms/step - loss: 0.0292 - RMSE: 0.1486 - val_loss: 0.0292 - val_RMSE: 0.1468
Epoch 23/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0298 - RMSE: 0.1507 - val_loss: 0.0285 - val_RMSE: 0.1440
Epoch 24/40
100/100 [==============================] - 0s 3ms/step - loss: 0.0277 - RMSE: 0.1444 - val_loss: 0.0309 - val_RMSE: 0.1519
Epoch 25/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0268 - RMSE: 0.1412 - val_loss: 0.0296 - val_RMSE: 0.1483
Epoch 26/40
100/100 [==============================] - 0s 3ms/step - loss: 0.0272 - RMSE: 0.1436 - val_loss: 0.0278 - val_RMSE: 0.1423
Epoch 27/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0268 - RMSE: 0.1412 - val_loss: 0.0279 - val_RMSE: 0.1427
Epoch 28/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0267 - RMSE: 0.1418 - val_loss: 0.0284 - val_RMSE: 0.1452
Epoch 29/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0266 - RMSE: 0.1421 - val_loss: 0.0264 - val_RMSE: 0.1382
Epoch 30/40
100/100 [==============================] - 0s 3ms/step - loss: 0.0255 - RMSE: 0.1381 - val_loss: 0.0278 - val_RMSE: 0.1436
Epoch 31/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0245 - RMSE: 0.1346 - val_loss: 0.0301 - val_RMSE: 0.1513
Epoch 32/40
100/100 [==============================] - 0s 3ms/step - loss: 0.0251 - RMSE: 0.1371 - val_loss: 0.0276 - val_RMSE: 0.1432
Epoch 33/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0256 - RMSE: 0.1389 - val_loss: 0.0277 - val_RMSE: 0.1431
Epoch 34/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0244 - RMSE: 0.1345 - val_loss: 0.0270 - val_RMSE: 0.1406
Epoch 35/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0244 - RMSE: 0.1358 - val_loss: 0.0264 - val_RMSE: 0.1387
Epoch 36/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0244 - RMSE: 0.1350 - val_loss: 0.0269 - val_RMSE: 0.1410
Epoch 37/40
100/100 [==============================] - 0s 3ms/step - loss: 0.0236 - RMSE: 0.1326 - val_loss: 0.0254 - val_RMSE: 0.1357
Epoch 38/40
100/100 [==============================] - 0s 3ms/step - loss: 0.0241 - RMSE: 0.1345 - val_loss: 0.0270 - val_RMSE: 0.1416
Epoch 39/40
100/100 [==============================] - 1s 6ms/step - loss: 0.0241 - RMSE: 0.1347 - val_loss: 0.0279 - val_RMSE: 0.1452
Epoch 40/40
100/100 [==============================] - 0s 4ms/step - loss: 0.0232 - RMSE: 0.1308 - val_loss: 0.0259 - val_RMSE: 0.1383

============================ Beginning trial with hyperparams: OPTIMIZER: Adam, LEARNING RATE: 0.001, EPOCHS: 40============================

Epoch 1/40
25/25 [==============================] - 1s 7ms/step - loss: 0.4225 - RMSE: 0.6193 - val_loss: 0.2406 - val_RMSE: 0.4784
Epoch 2/40
25/25 [==============================] - 0s 5ms/step - loss: 0.1520 - RMSE: 0.3761 - val_loss: 0.2532 - val_RMSE: 0.4916
Epoch 3/40
25/25 [==============================] - 0s 7ms/step - loss: 0.1141 - RMSE: 0.3240 - val_loss: 0.2445 - val_RMSE: 0.4834
Epoch 4/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0931 - RMSE: 0.2910 - val_loss: 0.2289 - val_RMSE: 0.4669
Epoch 5/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0863 - RMSE: 0.2793 - val_loss: 0.2175 - val_RMSE: 0.4547
Epoch 6/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0789 - RMSE: 0.2667 - val_loss: 0.2049 - val_RMSE: 0.4410
Epoch 7/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0683 - RMSE: 0.2458 - val_loss: 0.1859 - val_RMSE: 0.4192
Epoch 8/40
25/25 [==============================] - 0s 5ms/step - loss: 0.0660 - RMSE: 0.2409 - val_loss: 0.1696 - val_RMSE: 0.4001
Epoch 9/40
25/25 [==============================] - 0s 6ms/step - loss: 0.0636 - RMSE: 0.2359 - val_loss: 0.1508 - val_RMSE: 0.3767
Epoch 10/40
25/25 [==============================] - 0s 5ms/step - loss: 0.0590 - RMSE: 0.2262 - val_loss: 0.1388 - val_RMSE: 0.3601
Epoch 11/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0565 - RMSE: 0.2212 - val_loss: 0.1127 - val_RMSE: 0.3223
Epoch 12/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0550 - RMSE: 0.2174 - val_loss: 0.1060 - val_RMSE: 0.3113
Epoch 13/40
25/25 [==============================] - 0s 5ms/step - loss: 0.0525 - RMSE: 0.2117 - val_loss: 0.0913 - val_RMSE: 0.2866
Epoch 14/40
25/25 [==============================] - 0s 5ms/step - loss: 0.0509 - RMSE: 0.2084 - val_loss: 0.0801 - val_RMSE: 0.2671
Epoch 15/40
25/25 [==============================] - 0s 4ms/step - loss: 0.0473 - RMSE: 0.1996 - val_loss: 0.0678 - val_RMSE: 0.2433
Epoch 16/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0466 - RMSE: 0.1975 - val_loss: 0.0569 - val_RMSE: 0.2207
Epoch 17/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0464 - RMSE: 0.1973 - val_loss: 0.0519 - val_RMSE: 0.2098
Epoch 18/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0443 - RMSE: 0.1923 - val_loss: 0.0450 - val_RMSE: 0.1926
Epoch 19/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0416 - RMSE: 0.1852 - val_loss: 0.0405 - val_RMSE: 0.1800
Epoch 20/40
25/25 [==============================] - 0s 9ms/step - loss: 0.0425 - RMSE: 0.1879 - val_loss: 0.0378 - val_RMSE: 0.1732
Epoch 21/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0412 - RMSE: 0.1842 - val_loss: 0.0342 - val_RMSE: 0.1618
Epoch 22/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0394 - RMSE: 0.1797 - val_loss: 0.0334 - val_RMSE: 0.1608
Epoch 23/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0375 - RMSE: 0.1743 - val_loss: 0.0319 - val_RMSE: 0.1559
Epoch 24/40
25/25 [==============================] - 0s 5ms/step - loss: 0.0372 - RMSE: 0.1738 - val_loss: 0.0314 - val_RMSE: 0.1535
Epoch 25/40
25/25 [==============================] - 0s 8ms/step - loss: 0.0361 - RMSE: 0.1706 - val_loss: 0.0307 - val_RMSE: 0.1514
Epoch 26/40
25/25 [==============================] - 0s 4ms/step - loss: 0.0363 - RMSE: 0.1709 - val_loss: 0.0307 - val_RMSE: 0.1518
Epoch 27/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0356 - RMSE: 0.1689 - val_loss: 0.0295 - val_RMSE: 0.1471
Epoch 28/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0362 - RMSE: 0.1713 - val_loss: 0.0302 - val_RMSE: 0.1500
Epoch 29/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0354 - RMSE: 0.1684 - val_loss: 0.0296 - val_RMSE: 0.1485
Epoch 30/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0331 - RMSE: 0.1622 - val_loss: 0.0294 - val_RMSE: 0.1487
Epoch 31/40
25/25 [==============================] - 0s 5ms/step - loss: 0.0314 - RMSE: 0.1567 - val_loss: 0.0295 - val_RMSE: 0.1479
Epoch 32/40
25/25 [==============================] - 0s 7ms/step - loss: 0.0328 - RMSE: 0.1610 - val_loss: 0.0301 - val_RMSE: 0.1502
Epoch 33/40
25/25 [==============================] - 0s 5ms/step - loss: 0.0346 - RMSE: 0.1661 - val_loss: 0.0294 - val_RMSE: 0.1469
Epoch 34/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0337 - RMSE: 0.1640 - val_loss: 0.0300 - val_RMSE: 0.1517
Epoch 35/40
25/25 [==============================] - 0s 5ms/step - loss: 0.0315 - RMSE: 0.1569 - val_loss: 0.0292 - val_RMSE: 0.1483
Epoch 36/40
25/25 [==============================] - 0s 4ms/step - loss: 0.0322 - RMSE: 0.1594 - val_loss: 0.0294 - val_RMSE: 0.1494
Epoch 37/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0317 - RMSE: 0.1575 - val_loss: 0.0290 - val_RMSE: 0.1480
Epoch 38/40
25/25 [==============================] - 0s 3ms/step - loss: 0.0305 - RMSE: 0.1539 - val_loss: 0.0293 - val_RMSE: 0.1482
Epoch 39/40
25/25 [==============================] - 0s 6ms/step - loss: 0.0305 - RMSE: 0.1544 - val_loss: 0.0284 - val_RMSE: 0.1466
Epoch 40/40
25/25 [==============================] - 0s 6ms/step - loss: 0.0306 - RMSE: 0.1552 - val_loss: 0.0293 - val_RMSE: 0.1490

============================ Beginning trial with hyperparams: OPTIMIZER: Adam, LEARNING RATE: 0.001, EPOCHS: 40============================

Epoch 1/40
13/13 [==============================] - 1s 12ms/step - loss: 0.8698 - RMSE: 0.8862 - val_loss: 0.3136 - val_RMSE: 0.5491
Epoch 2/40
13/13 [==============================] - 0s 4ms/step - loss: 0.3266 - RMSE: 0.5503 - val_loss: 0.2610 - val_RMSE: 0.4990
Epoch 3/40
13/13 [==============================] - 0s 8ms/step - loss: 0.1986 - RMSE: 0.4325 - val_loss: 0.2437 - val_RMSE: 0.4799
Epoch 4/40
13/13 [==============================] - 0s 8ms/step - loss: 0.1593 - RMSE: 0.3834 - val_loss: 0.2396 - val_RMSE: 0.4762
Epoch 5/40
13/13 [==============================] - 0s 5ms/step - loss: 0.1373 - RMSE: 0.3569 - val_loss: 0.2434 - val_RMSE: 0.4815
Epoch 6/40
13/13 [==============================] - 0s 4ms/step - loss: 0.1203 - RMSE: 0.3355 - val_loss: 0.2367 - val_RMSE: 0.4763
Epoch 7/40
13/13 [==============================] - 0s 4ms/step - loss: 0.1073 - RMSE: 0.3127 - val_loss: 0.2289 - val_RMSE: 0.4695
Epoch 8/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0929 - RMSE: 0.2900 - val_loss: 0.2290 - val_RMSE: 0.4694
Epoch 9/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0858 - RMSE: 0.2787 - val_loss: 0.2181 - val_RMSE: 0.4586
Epoch 10/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0861 - RMSE: 0.2793 - val_loss: 0.2064 - val_RMSE: 0.4463
Epoch 11/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0857 - RMSE: 0.2787 - val_loss: 0.2013 - val_RMSE: 0.4405
Epoch 12/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0808 - RMSE: 0.2703 - val_loss: 0.1926 - val_RMSE: 0.4307
Epoch 13/40
13/13 [==============================] - 0s 5ms/step - loss: 0.0726 - RMSE: 0.2560 - val_loss: 0.1844 - val_RMSE: 0.4219
Epoch 14/40
13/13 [==============================] - 0s 10ms/step - loss: 0.0706 - RMSE: 0.2510 - val_loss: 0.1796 - val_RMSE: 0.4160
Epoch 15/40
13/13 [==============================] - 0s 8ms/step - loss: 0.0672 - RMSE: 0.2432 - val_loss: 0.1674 - val_RMSE: 0.4018
Epoch 16/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0657 - RMSE: 0.2398 - val_loss: 0.1583 - val_RMSE: 0.3908
Epoch 17/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0632 - RMSE: 0.2364 - val_loss: 0.1509 - val_RMSE: 0.3812
Epoch 18/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0631 - RMSE: 0.2347 - val_loss: 0.1416 - val_RMSE: 0.3687
Epoch 19/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0609 - RMSE: 0.2294 - val_loss: 0.1331 - val_RMSE: 0.3577
Epoch 20/40
13/13 [==============================] - 0s 5ms/step - loss: 0.0566 - RMSE: 0.2223 - val_loss: 0.1238 - val_RMSE: 0.3442
Epoch 21/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0568 - RMSE: 0.2224 - val_loss: 0.1167 - val_RMSE: 0.3331
Epoch 22/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0555 - RMSE: 0.2173 - val_loss: 0.1095 - val_RMSE: 0.3221
Epoch 23/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0551 - RMSE: 0.2175 - val_loss: 0.1031 - val_RMSE: 0.3113
Epoch 24/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0519 - RMSE: 0.2117 - val_loss: 0.0973 - val_RMSE: 0.3020
Epoch 25/40
13/13 [==============================] - 0s 5ms/step - loss: 0.0517 - RMSE: 0.2085 - val_loss: 0.0892 - val_RMSE: 0.2882
Epoch 26/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0471 - RMSE: 0.1992 - val_loss: 0.0859 - val_RMSE: 0.2820
Epoch 27/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0490 - RMSE: 0.2045 - val_loss: 0.0772 - val_RMSE: 0.2664
Epoch 28/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0457 - RMSE: 0.1950 - val_loss: 0.0701 - val_RMSE: 0.2525
Epoch 29/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0474 - RMSE: 0.2009 - val_loss: 0.0678 - val_RMSE: 0.2481
Epoch 30/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0454 - RMSE: 0.1961 - val_loss: 0.0620 - val_RMSE: 0.2369
Epoch 31/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0450 - RMSE: 0.1939 - val_loss: 0.0588 - val_RMSE: 0.2297
Epoch 32/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0429 - RMSE: 0.1896 - val_loss: 0.0544 - val_RMSE: 0.2190
Epoch 33/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0429 - RMSE: 0.1905 - val_loss: 0.0511 - val_RMSE: 0.2125
Epoch 34/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0423 - RMSE: 0.1888 - val_loss: 0.0495 - val_RMSE: 0.2091
Epoch 35/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0410 - RMSE: 0.1832 - val_loss: 0.0455 - val_RMSE: 0.1986
Epoch 36/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0414 - RMSE: 0.1857 - val_loss: 0.0434 - val_RMSE: 0.1931
Epoch 37/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0422 - RMSE: 0.1886 - val_loss: 0.0415 - val_RMSE: 0.1886
Epoch 38/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0390 - RMSE: 0.1785 - val_loss: 0.0406 - val_RMSE: 0.1850
Epoch 39/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0400 - RMSE: 0.1807 - val_loss: 0.0365 - val_RMSE: 0.1739
Epoch 40/40
13/13 [==============================] - 0s 4ms/step - loss: 0.0397 - RMSE: 0.1828 - val_loss: 0.0351 - val_RMSE: 0.1701</code></pre>
</div>
</div>
<div class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>final_metrics <span class="op">=</span> []</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>best_hyperparam <span class="op">=</span> <span class="va">None</span><span class="op">;</span> lowest_loss <span class="op">=</span> <span class="dv">10000</span><span class="op">;</span> idx <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> mdl, history <span class="kw">in</span> models:</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>    final_metrics.append(history.history[<span class="st">"val_RMSE"</span>][<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (hparam, metric) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(hyperparam_combinations, final_metrics)):</span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Optimizer: </span><span class="sc">{</span>hparam[<span class="dv">0</span>]<span class="sc">:&gt;5}</span><span class="ss">| LR: </span><span class="sc">{</span>hparam[<span class="dv">1</span>]<span class="sc">:.4f}</span><span class="ss">| Epochs: </span><span class="sc">{</span>hparam[<span class="dv">2</span>]<span class="sc">}</span><span class="ss">| Batch Size: </span><span class="sc">{</span>hparam[<span class="dv">3</span>]<span class="sc">:3d}</span><span class="ss">| Validation RMSE: </span><span class="sc">{</span>metric<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> lowest_loss <span class="op">&gt;</span> metric:</span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a>        lowest_loss <span class="op">=</span> metric</span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a>        best_hyperparam <span class="op">=</span> hparam</span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">=</span> i</span>
<span id="cb69-13"><a href="#cb69-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb69-14"><a href="#cb69-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Best Combination: </span><span class="sc">{</span>best_hyperparam<span class="sc">}</span><span class="ss">| Corresponding Validation RMSE: </span><span class="sc">{</span>lowest_loss<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Optimizer:   SGD| LR: 0.0500| Epochs: 20| Batch Size:   8| Validation RMSE: 0.141
Optimizer:   SGD| LR: 0.0500| Epochs: 20| Batch Size:  32| Validation RMSE: 0.159
Optimizer:   SGD| LR: 0.0500| Epochs: 20| Batch Size:  64| Validation RMSE: 0.263
Optimizer:   SGD| LR: 0.0500| Epochs: 40| Batch Size:   8| Validation RMSE: 0.137
Optimizer:   SGD| LR: 0.0500| Epochs: 40| Batch Size:  32| Validation RMSE: 0.143
Optimizer:   SGD| LR: 0.0500| Epochs: 40| Batch Size:  64| Validation RMSE: 0.163
Optimizer:   SGD| LR: 0.0010| Epochs: 20| Batch Size:   8| Validation RMSE: 0.257
Optimizer:   SGD| LR: 0.0010| Epochs: 20| Batch Size:  32| Validation RMSE: 0.327
Optimizer:   SGD| LR: 0.0010| Epochs: 20| Batch Size:  64| Validation RMSE: 0.474
Optimizer:   SGD| LR: 0.0010| Epochs: 40| Batch Size:   8| Validation RMSE: 0.223
Optimizer:   SGD| LR: 0.0010| Epochs: 40| Batch Size:  32| Validation RMSE: 0.285
Optimizer:   SGD| LR: 0.0010| Epochs: 40| Batch Size:  64| Validation RMSE: 0.331
Optimizer:  Adam| LR: 0.0500| Epochs: 20| Batch Size:   8| Validation RMSE: 0.174
Optimizer:  Adam| LR: 0.0500| Epochs: 20| Batch Size:  32| Validation RMSE: 0.141
Optimizer:  Adam| LR: 0.0500| Epochs: 20| Batch Size:  64| Validation RMSE: 0.135
Optimizer:  Adam| LR: 0.0500| Epochs: 40| Batch Size:   8| Validation RMSE: 0.152
Optimizer:  Adam| LR: 0.0500| Epochs: 40| Batch Size:  32| Validation RMSE: 0.147
Optimizer:  Adam| LR: 0.0500| Epochs: 40| Batch Size:  64| Validation RMSE: 0.139
Optimizer:  Adam| LR: 0.0010| Epochs: 20| Batch Size:   8| Validation RMSE: 0.155
Optimizer:  Adam| LR: 0.0010| Epochs: 20| Batch Size:  32| Validation RMSE: 0.170
Optimizer:  Adam| LR: 0.0010| Epochs: 20| Batch Size:  64| Validation RMSE: 0.324
Optimizer:  Adam| LR: 0.0010| Epochs: 40| Batch Size:   8| Validation RMSE: 0.138
Optimizer:  Adam| LR: 0.0010| Epochs: 40| Batch Size:  32| Validation RMSE: 0.149
Optimizer:  Adam| LR: 0.0010| Epochs: 40| Batch Size:  64| Validation RMSE: 0.170

Best Combination: ('Adam', 0.05, 20, 64)| Corresponding Validation RMSE: 0.135</code></pre>
</div>
</div>
<p>Justify your choice of optimizers and regulizations used and the hyperparameters tuned</p>
<p>Score: 4 Marks</p>
<div class="alert alert-primary" style="padding: 10px; background-color: #a5eaf2; color: gray;">
<pre><code>&lt;b&gt;Optimizer Used&lt;/b&gt;&lt;br&gt;
&lt;span&gt;SGD/Adam both were tried, they give comparitively similar performance.&lt;/span&gt;&lt;br&gt;
&lt;b&gt;Regularization Used&lt;/b&gt;
&lt;span&gt;L1 Regularization with a strength of 1e-4. L1 could completely shut off an unnecessary neuron which might not be necessary thereby performing implicit feature selection.&lt;/span&gt;&lt;br&gt;
&lt;b&gt;Hyperparams Tuned&lt;/b&gt;
&lt;ul&gt;
    &lt;li&gt;&lt;b&gt;Optimizer&lt;/b&gt;: Adam/ SGD&lt;/li&gt;
    &lt;li&gt;&lt;b&gt;Number of Epochs&lt;/b&gt;: 20, 40&lt;/li&gt;
    &lt;li&gt;&lt;b&gt;Learning Rate&lt;/b&gt;: 0.05, 0.001&lt;/li&gt;
    &lt;li&gt;&lt;b&gt;Batch Size&lt;/b&gt;: 8, 32, 64&lt;/li&gt;
&lt;/ul&gt;</code></pre>
</div>
</section>
</section>
<section id="test-the-model" class="level1">
<h1>6. Test the model</h1>
<p>Score: 2 Marks</p>
<div class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the best model</span></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>mdl, history <span class="op">=</span> models[idx]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the predictions for the test</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> mdl.predict(X_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>7/7 [==============================] - 0s 1ms/step</code></pre>
</div>
</div>
<div class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the rmses for all the three tests individually</span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>squared_diff <span class="op">=</span> (y_test <span class="op">-</span> predictions) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>sum_squared_diff <span class="op">=</span> squared_diff.<span class="bu">sum</span>(axis <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>mean_squared_diff <span class="op">=</span> sum_squared_diff <span class="op">/</span> y_test.shape[<span class="dv">0</span>]</span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>individual_rmses <span class="op">=</span> mean_squared_diff <span class="op">**</span> <span class="fl">0.5</span></span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> subject, metric <span class="kw">in</span> <span class="bu">zip</span>(target_variables, individual_rmses):</span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"In </span><span class="sc">{</span>subject<span class="sc">}</span><span class="ss"> our model is off by around </span><span class="sc">{</span><span class="dv">100</span> <span class="op">*</span> metric<span class="sc">:.2f}</span><span class="ss"> marks on average"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>In math score our model is off by around 14.65 marks on average
In reading score our model is off by around 13.75 marks on average
In writing score our model is off by around 13.64 marks on average</code></pre>
</div>
</div>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">7. Conclusion</h2>
<p>Plot the training and validation loss Report the testing accuracy and loss.</p>
<p>Report values for preformance study metrics like accuracy, precision, recall, F1 Score.</p>
<p>A proper comparision based on different metrics should be done and not just accuracy alone, only then the comparision becomes authentic. You may use Confusion matrix, classification report, MAE etc per the requirement of your application/problem.</p>
<p>Score 2 Marks</p>
<div class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the Loss and Metric curves for best model trained</span></span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize <span class="op">=</span> (<span class="dv">10</span>,<span class="dv">6</span>), sharex <span class="op">=</span> <span class="va">True</span>, sharey <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> ax.flat</span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>train_loss <span class="op">=</span> history.history[<span class="st">"loss"</span>]<span class="op">;</span> train_metric <span class="op">=</span> history.history[<span class="st">"RMSE"</span>]</span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>valid_loss <span class="op">=</span> history.history[<span class="st">"val_loss"</span>]<span class="op">;</span> valid_metric <span class="op">=</span> history.history[<span class="st">"val_RMSE"</span>]</span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="bu">range</span>(<span class="bu">len</span>(train_loss))</span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(x, train_loss)<span class="op">;</span> ax[<span class="dv">0</span>].set_ylabel(<span class="st">"Training Loss"</span>)<span class="op">;</span> ax[<span class="dv">0</span>].set_title(<span class="st">"Training Loss Profile"</span>)</span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(x, valid_loss)<span class="op">;</span> ax[<span class="dv">1</span>].set_ylabel(<span class="st">"Validation Loss"</span>)<span class="op">;</span> ax[<span class="dv">1</span>].set_title(<span class="st">"Validation Loss Profile"</span>)</span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-12"><a href="#cb77-12" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].plot(x, train_metric)<span class="op">;</span> ax[<span class="dv">2</span>].set_xlabel(<span class="st">"Epochs"</span>)<span class="op">;</span> ax[<span class="dv">2</span>].set_ylabel(<span class="st">"Training RMSE"</span>)<span class="op">;</span> ax[<span class="dv">2</span>].set_title(<span class="st">"Training Metric Profile"</span>)</span>
<span id="cb77-13"><a href="#cb77-13" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">3</span>].plot(x, valid_metric)<span class="op">;</span> ax[<span class="dv">3</span>].set_xlabel(<span class="st">"Epochs"</span>)<span class="op">;</span> ax[<span class="dv">3</span>].set_ylabel(<span class="st">"Validation RMSE"</span>)<span class="op">;</span> ax[<span class="dv">3</span>].set_title(<span class="st">"Validation Metric Profile"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="DL_Assignment_Group_97_files/figure-html/cell-51-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the regression metrics for the validation set</span></span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>percent_error <span class="op">=</span> np.<span class="bu">abs</span>(predictions <span class="op">-</span> y_test) </span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>mae <span class="op">=</span> percent_error.mean(axis <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>mse <span class="op">=</span> ((predictions <span class="op">-</span> y_test) <span class="op">**</span> <span class="dv">2</span>).mean(axis <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>rmse <span class="op">=</span> mse <span class="op">**</span> <span class="fl">0.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the metrics in a dataframe</span></span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(np.vstack([mse, rmse, mae]))</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>df.index <span class="op">=</span> [<span class="st">"MSE"</span>, <span class="st">"RMSE"</span>, <span class="st">"MAE"</span>]</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>df.columns <span class="op">=</span> target_variables</span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="52">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>math score</th>
      <th>reading score</th>
      <th>writing score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>MSE</th>
      <td>0.021477</td>
      <td>0.018918</td>
      <td>0.018592</td>
    </tr>
    <tr>
      <th>RMSE</th>
      <td>0.146550</td>
      <td>0.137543</td>
      <td>0.136351</td>
    </tr>
    <tr>
      <th>MAE</th>
      <td>0.116321</td>
      <td>0.108885</td>
      <td>0.103714</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="alert alert-primary" style="padding: 10px; background-color: #a5eaf2; color: gray;">
<p>Since this problem is a regression problem, we are not computing metrics like accuracy, recall etc. We are only computing the Regression specific metrics and it seems like our models are off by 10-15 marks on an average in predicting the scores across the three different tests…</p>
</div>
</section>
<section id="solution" class="level2">
<h2 class="anchored" data-anchor-id="solution">8. Solution</h2>
<p>What is the solution that is proposed to solve the business problem discussed in Section 1. Also share your learnings while working through solving the problem in terms of challenges, observations, decisions made etc.</p>
<p>Score 2 Marks</p>
<div class="alert alert-primary" style="padding: 10px; background-color: #a5eaf2; color: gray;">
<pre><code>&lt;span&gt;&lt;b&gt;Business Solution&lt;/b&gt;: In this project we created a model to predict the performance of a student on a test given some pre-known attributes about a subject/student. Using this model and the attributes, we could figure out which students are likely to perform poorly or well in a particular field such as math or reading etc. This information could be utilized in several ways&lt;/span&gt;
&lt;ul&gt;
    &lt;li&gt;We could form groups or pairs of students who are strong in one subject and weak in another to encourage peer learning&lt;/li&gt;
    &lt;li&gt;For those students whose scores are predicted to be too poor in a particular subject, extra attention could be given to understand their concerns, challenges and make the course experience better for them&lt;/li&gt;
    &lt;li&gt;We could understand the strength of individual candidates and provide special attention to help them pursue those fields more agressively as per their capacity and predilection.&lt;/li&gt; 
&lt;/ul&gt;
&lt;b&gt;Challenges Faced&lt;/b&gt;
&lt;ul&gt;
    &lt;li&gt;Exhaustive hyperparameter tuning is very difficult for neural networks. We just used a small subset of most influential hyperparams to our knowledge for getting the most optimal model&lt;/li&gt;
    &lt;li&gt;The same goes for decision of model architecture. We also tried using a single hidden layers with a more neurons but it didn't supercede the performance of a deeper but less tall network&lt;/li&gt;
    &lt;li&gt; We couldn't think of any feature engineering methods by combining several features or something like that. Also all the features are categorical in nature which naturally limits any mathematical transformations like square, log, groupwise average etc.&lt;/li&gt;
&lt;/ul&gt;    
&lt;b&gt;Future Scope&lt;/b&gt;
&lt;ul&gt;
    &lt;li&gt;Collect/Simulate more data examples as the number of records are pretty low.&lt;/li&gt;
    &lt;li&gt;Instead of plain one-hot/label encoding techniques, we could use Embeddings to represent categorical features which could be conveniently learned as per the task at hand&lt;/li&gt;
    &lt;li&gt;Doing more extensive hyperparameter tuning and training with some tricks like learning rate scheduling etc.&lt;/li&gt;
&lt;/ul&gt;</code></pre>
</div>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>